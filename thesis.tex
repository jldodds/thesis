\documentclass{puthesis}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{url}       % SB
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{times}
\usepackage{xcolor}
\usepackage{textcomp}
\usepackage{mathpartir}
\usepackage{semantic}
\usepackage{listings}
\usepackage{lstlangcoq}
\usepackage{caption}
\usepackage{stmaryrd}
\renewcommand{\lstlistingname}{Figure}
\input{macros}

\lstset{language=Coq,basicstyle=\sffamily,mathescape=true,columns=fullflexible}



\author{Josiah Dodds}
\adviser{Andrew Appel}
\title{Computation Improves Interactive Symbolic Execution}
\abstract{The abstract goes here.}
\acknowledgements{Thank you very much.}
\dedication{To myself.}



\begin{document}



\chapter{Introduction}

The C programming language is one of the most commonly used languages
for writing critical software. C is often used for operating systems,
embedded systems, and cryptography, as well as countless libraries
that want great performance and a reasonably common interface. Because
there is so much critical C code, much of the world depends on the
correctness of that code for almost every aspect of their lives.

In general, the code we depend on works well, but when it fails it can
be catastrophic. Space projects that cost hundreds of millions of
dollars have failed due to software failures, radiation devices with
software flaws have overdosed cancer patients, nuclear missile
detection systems have given false positives, and millions of cars
have been recalled to fix software bugs. Those scenarios are only
bugs; Far more systems can fail when a malicious user attacks them,
especially in a time where every aspect of our life contains
computers. As an example a brand of refrigerators was found to have
been taken over to send spam email. Developing tools that can increase
confidence in C programs will help increase confidence in many of
these vulnerable/critical systems.

To believe that a C program will run correctly we must first believe a
number of other things. There are a number of components that we will
need to be able to \emph{trust} in order to trust our C program. need
to fully understand the program itself, and believe that when executed
it will have the expected behavior. We also need to believe that when
the C code is translated to machine language, the generated machine
language has the same behavior as the original program. Finally we
need to believe that the machine executing the machine language is
doing the correct thing. We call the collection of things we must
trust the \emph{trusted computing base}. In general, the larger the
trusted computing base is, the harder it is to be confident that the
program is correct.

The need for trust follows a chain. It starts with the C program at
the top, the most human understandable part of the chain, and moves
down to the actual execution of the machine. In order to make the
trusted computing base as small as possible, we must maximize our
confidence in every step of the process.

Proof assistants are tools that help to write and check mathematical
proofs. If you want to believe a classic mathematical proof you need
to understand the statement of the proof, which is open to ambiguities
based on assumptions made by the writer, and then believe every single
step of the proof itself, even steps where the proof author says
something like ``clearly'' or ``obviously''. This can be an enormous
burden on the person that needs to understand how the proof was done;
New proofs can use new techniques that are impenetrable to anyone but
experts in the specific domain the proof is about. A proof assistant,
on the other hand, forces proof statements to be written using a set
of basic, well defined building blocks, making theorem statements
completely unambiguous. The proof assistant provides tools for proving
theorems, but those tools don't need to be trusted. All that needs to
be trusted is a part of the proof assistant that checks the proof when
it is created. This is advantageous because the proof itself becomes
irrelevant to someone interested in believing the theorem. All that is
relevant that the proof assistant successfully checked the proof, and
that the proof assistant's checker is correct. In other words, the
trusted base of a theorem proved in a proof assistant is the statement
of that theorem and the correctness of the checker, but we have very
strong trust in the correctness of the checker.

Peer review is the technique by which mathematical proofs are judged
to be correct. A number of people read the proof and if they feel that
every step in the proof is correct, a consensus might eventually be
reached that the proof is actually correct. This must be done for
every theorem that will ever be proved. With a proof assistant though,
all that needs to be peer reviewed is the statement of the theorem,
which should be understandable to anyone that is interested in its
results, and the proof checker, which is of interest to anyone who
wants to believe a proof. So instead of a subset of a specific
community reviewing any given proof, a proof assistant is reviewed by
a good fraction of the entire community that is interested in proofs.

A verified compiler gives us a proof that if it is given a valid C
program, it will generate assembly code with the same behavior as the
C program. This proof is checked by the Coq proof assistant, meaning
that its trusted base is the Coq proof checker, and the statement of
the theorem. To be able to use the theorem effectively though, we need
to be able to show that the C program will always have a valid
execution.

If we showed that the C program executed successfully, the verified
compiler would guarantee that the assembly code that it outputs has
the same behavior as the C input. This is an important step, but it
can be very hard to determine what a C program is doing just by
examining the source code.  A typical C program doesn't just specify
what it is doing, it specifies how it is doing it. In C, the
specification is relatively low level meaning that a program will need
more details about how the computation is being done than a higher
level language. This is part of why C is so successful, the added
control allows programmers to write more efficient programs, but it
can make it much harder to understand what the program is actually
doing.

In order to specify what a program is doing rather than how it is
doing it we write a mathematical specification of the program should
do. This mathematical model lacks the complexity that a real program
needs to work efficiently on a computer. The specification is a new
link in the chain above the C source code, but it is not attached
yet. A \emph{Program Logic} is used to prove that the result of
executing the C program meets the specification. This program logic
can be written in Coq, and proved correct using the same specification
of the C language that CompCert uses. With this proof, the chain can
be completed from specification all the way to assembly language.

The combination of the tools mentioned here gives proof that the
assembly code that the machine executes has the same behavior as an
abstract mathematical specification. To believe that the proof is
correct, all we need to trust is the mathematical specification itself
and the theorem proverb'sproof checker.

There is still a significant amount of work to do if you wish to prove
a program correct. First you must create an accurate specification
which can be difficult, especially for complex programs. Then the
process of proving a program meets a specification using a program
logic can be very involved. Program logics implemented in proof
assistants are often \emph{interactive}. To build a proof in an
interactive program logic, you examine a proof state and then perform
an action to manipulate the state. The action will result in a new
proof state, which you can manipulate again, and the process repeats
until the proof is completed.

One of the biggest problems that complex program logics run into is
that they are slow. This means that the time between giving input and
receiving a new proof state can be long. Writing a proof in a program
logic is already difficult work, requiring intimate knowledge of the
program, the programming language, and the logic. Having to wait
between each step in a proof while the program does computation
multiplies the difficulty of the proof. This is especially true
because proof development is often experimental in nature. The first
step is to make a good guess at a specification. If the proof gets
stuck, either the specification or the program needs to change. If
either of those change, the proof will need to start over from the
beginning. A slow program logic can drastically limit the number of
manipulations of the program or the specification that can be done in
any given stretch of time. If the time is long enough, it can also be
hard for the person doing the proof to remember what they were working
on when they last made progress on the proof.

This thesis shows how a complex programming logic for proving the
correctness of C programs can be made significantly faster without any
noticeable impact on the proving experience. This results in a logic
that is significantly more intuitive and usable, and is a vital step
in making the program logic a widely usable tool.

Many of the parts discussed so far are existing work: The proof
assistant we have chosen to use is Coq, Leroy's CompCert \cite{} is a
C compiler implemented and proved sound in Coq, and Appel et al. have
created the Verified Software Toolchain to give secure chain from
specification language to assembly including a specification logic and
a program logic to relate the specification to a CompCert C program.


\chapter{Computation in Coq}

The Coq proof assistant has a built in functional programming language
called Gallina with the ability to match inductive data-structures. The type
boolean, for example can be defined:

\begin{lstlisting}
Inductive boolean :=
| true
| false.
\end{lstlisting}

so \lstinline|boolean| is a type that can be constructed by either
\lstinline|true| or \lstinline|false| and when given a
\lstinline|boolean|, a function can decide which of the two it is. Now
we can look at the definition of \lstinline|True| and
\lstinline|False| (capitalized), which are in the type
\lstinline|Prop| instead of \lstinline|bool|. Prop is not an
inductively defined type in Coq, so it can't be matched
against. Instead the types of \lstinline|True| and \lstinline|False|
are

\begin{lstlisting}
Inductive True : Prop := I : True.

Inductive False : Prop
\end{lstlisting}

So if a Coq program is given something of type \lstinline|Prop|, it
has no ability to determine what it is, just like if it is given a
value of an arbitrary type it is unable to determine what that type
is. That doesn't mean that it is impossible to reason about things of
type \lstinline|Prop| in Coq though. Instead, Coq has tactics, which
exist almost exclusively for that reason. A tactic in Coq is a program
that manipulates a proof state. A Coq proof state is a goal, or a
\lstinline|Prop| to be proved, along with a number of quantified
variables of any type, and a number of hypothesis of type
\lstinline|Prop|. Because Ltac is used to build proofs, it must keep a
record any time it makes progress on the proof term so that when the
proof is over, the record of the proof, or the proof object, can be
checked.

This design means that Ltac doesn't need to be sound. As long as it
results in a correct proof in the end, it doesn't really matter how it
did it. Proofs that do a large amount of proof search to do a
reasonably small amount of work in the end can perform reasonably well
in Ltac. Unfortunately, many of the proofs that Coq's users want to do
require both a lot of proof search, and numerous operations over the
proof state to succeed. In these cases, the overhead of keeping and
modifying the
proof object (which can take up substantial amounts of memory) can
lead to very slow Ltac performance. 

Logics such as VST's verifiable C logic are in
\lstinline|Prop|. Logics in Prop are said to be shallowly
embedded. They use the syntax of Coq, rather than defining a logic of
their own. Some systems such as Appel's VeriSmall \cite{} are deeply
embedded, meaning they define their own syntax, along with a
denotation that gives meaning to that syntax in Coq's logic. 

Shallow and Deep embeddings have tradeoffs in two areas:

\begin{enumerate}
\item interactivity, or how convenient it is for users to interact
  with the logic, generally in the way they would expect to be able to
  interact with Coq's logic, and
\item automation, or the ability of the logic writer to provide the
  user of the logic with tools to efficiently reason about the logic.
\end{enumerate}

For interactivity, a shallow embedding will automatically get all of
the automation that Coq provides. Ltac can be used to write decision
procedures about the logic without requiring soundness proofs for
those procedures. A deep embedding, however, will be much harder to
interact with. To have meaning as a proof goal, a deep embedding will
need to be wrapped in its denotation function. At this point many of
the operations users familiar with Coq would expect will require a
significant amount of work from the creator of the logic. The main
reason for this is that in a deep embedding every operation on the
deeply embedded assertion must be proved sound with respect to the
denotation function. Take Coq's rewrite tactic as an example. In this
tactic if we have a lemma about an equality for example:

\begin{lstlisting}
Lemma add_symm : forall a b, a + b = b + a
\end{lstlisting}

and we wish to prove a goal 

\begin{lstlisting}
a : nat
b : nat
c : nat
======================
a + (b + c) = a + (c + b) 
\end{lstlisting}

we can use the tactic \lstinline|rewrite (add_sym b c); reflexivity|
to transform the left side to match the right side, and then tell Coq
that we have an equation where the two sides are syntactically
equal. We supply the arguments \lstinline|b| and \lstinline|c| to the
tactic because it is ambiguous where to rewrite the lemma, and Coq
might guess wrong unless we tell it.

Now imagine we have a deep embedding for nat, addition, and equality

\begin{lstlisting}
Inductive expr' :=
| num : nat -> expr
| add : expr -> expr -> expr.

Inductive expr :=
| eq : expr' -> expr' -> expr.

Fixpoint expr'_denote e:=
match e with
| num n => n
| add e1 e2 => expr_denote e1 + expr_denote e2
end.

Definition expr_denote e :=
match e with
| eq e1 e2 => expr'_denote e1 = expr'_denote e2
end.
\end{lstlisting} 

Then we can write a symmetry lemma

\begin{lstlisting}
Lemma add_sym' : forall a b, 
expr'_denote (add a b) = expr'_denote (add b a)
\end{lstlisting}

While this isn't too hard to prove, it also doesn't end up being too
useful in proving our goal:

\begin{lstlisting}
a : expr
b : expr
c : expr
======================
expr_denote (add a (add b c)) (add a (add c b))  
\end{lstlisting}

The lemma we wrote doesn't match the syntax of what we need to prove
so we can't use the rewrite tactic. We could unfold the definition of
\lstinline|expr_denote| and \lstinline|expr_denote'| in our goal, but
then we would lose the deep embedding. Instead, if we want this
functionality, we need to write a \emph{Coq function} that does the
rewrite and prove that function sound with respect to the denotation
function (assuming we have an equality function on our expr, which
isn't hard to write):

\begin{lstlisting}
Fixpoint symmetry' e e1 e2 := 
match e with
| add e1' e2' => if (e1 == e1' && e2 = e2') 
                 then add e2' e1' 
                 else add (symmetry e1') 
                          (symmetry e2')
| x => x
end.

Definition symmetry e e1 e2 :=
match e with
| eq ex1 ex2 => eq (symmetry' ex1 e1 e2) (symmetry ex2 e1 e2)
end.

Lemma symmetry_func_sound : 
forall e1 e2,
expr_denote (e) = expr_denote (symmetry e)
...
\end{lstlisting}

Now we can rewrite by \lstinline|symmetry_func_sound| and simplify the definition
of symmetry in the new goal, and we will have done the same thing we
did with LTac in a single command.

Why would we ever use a deep embedding when automation is so much
work? The main reason is efficiency. While LTac does an operation and
needs to build a proof object, a proved-sound function that operates on
a deeply embedded proof goal does an operation and at the same time is
a (very small) proof object. The example above did a very small amount
of work in the symmetry function, but such a function could be an
entire decision procedure. Then we would be able to do a large
amount of work on a deeply embedded proof term while hardly generating
any proof object at all. In general, this will be substantially more
efficient than using LTac, especially as the size of the
deeply-embedded statements grows. 

Another advantage to a deeply embedded logic is that if there is a
single decision procedure for it that works all in one function, that
function can be \emph{extracted} and run in OCaml. This gives the
performance of an optimized language, with most of the assurance that
comes with having a proved-sound decision procedure in Coq.

The technique of computational reflection in Coq aims to combine the
interactivity of a shallow embedding with the efficiency of a deep
embedding by moving between the two when it is needed.

\section{Computational reflection}

Computational reflection is the process of translating a shallow
embedding to a deep embedding, applying a proved-sound function to the
deep embedding, then evaluating the denotation function to return to
the shallow embedding. If this is done all in a single step, the user
will never know that the technique of reflection is being used. That
means that the efficient, proved-sound decision procedures that can be
used on deep embeddings can essentially be used as if they are
tactics, along side all of the other tactics and LTac automation that
might be built up around a logic. 

The process of translating from a shallow embedding to a deep
embedding is known as reification. Because Coq programs can't match on
shallow embeddings, reification must be performed by a
tactic. Reification leaves us with a deep embedding (or reified
expression), which allows us to use proved-sound Coq functions to
progress the proof state without large proof terms. When the deep
embedding is at a state that requires human work, the denotation
function can be carefully unfolded, or reflected, to return it to a
shallow embedding that is easy for the user to view and work with.

%graphic from prefpo 

Many shallow embeddings, including VST, have parts that are made up
completely of constructors. In most program logics, for example, the
programming language will not be Coq's programming language, but a
deep embedding of the syntax of the language along with a semantics
that describes the execution of that syntax in the state of the
programming language. Because the syntax is a deep embedding, it is
possible to write Coq functions that reason about them, and then prove
those functions sound (usually with respect to the operational
semantics). This is a type of reflection that doesn't require
reification at all, making it more efficient and allowing it to fit
cleanly into the shallowly embedded program logic. One such example in
VST is the typechecker which is used to show that expressions in the
program successfully evaluate given the state represented by the
precondition.


\chapter{Typechecking C Expressions}

The CompCert operational semantics has a partial inductive relation
$\rho \vdash e\Downarrow v$ for evaluation of an ($r$-value)
expression $e$ to a value $v$ in environment $\rho$, and another
relation $\rho \vdash e\Downarrow_l v$ for the $l$-value
interpretation of an expression.  These relations are inconvenient in
program proof, as we would frequently need existential variables
$\exists v.~\rho \vdash e\Downarrow_l v$, and proofs of often trivial
side-conditions that $v$ exists.

C's type system is unsound---type-checking does not guarantee safe
execution. In general, it is impossible to have a sound typechecker
for C because the safe execution of C programs depends on dynamic
properties of the program. Other languages have error handling
mechanisms built into their semantics (such as exceptions). A sound
typechecker guarantees that the program will always have an execution
defined by the semantics, so a language with exceptions for run-time
errors can have a sound typechecker, as can a language without
run-time errors. 

To guarantee that a C program always has an execution that is defined
by the C semantics our type-checker for C \cite[Chapter
25]{appel14:plcc} \lstinline|tc_expr ($\Delta$ : tycontext) (e : expr) : tc_assert|

 calculates minimal nontrivial separation-logic
assertions required for safe evaluation of expressions.  It does this
with the help of a type-context $\Delta$ that (conservatively) tracks the
initialization status of all variables; soundness of the program logic
guarantees that all initialized variables contain defined values of
the right type. Because the typechecker will be a function whose
result we will use in our program logic, it is important that all of
its arguments will be deeply embedded. The expression comes from a
concrete compiled program, so we know that it will be made completely
of constructors. We chose the definition of the type context to be a
computationally efficient mapping structure indexed by program
variables that will always have concrete values. This means that we
can be confident that a call to \lstinline|tc_expr| will always compute
to an assertion.


If we know that an expression will successfully evaluate, we can
eliminate the existentials connected to expression evaluation by
defining (total) functions \lstinline|eval_expr| and
\lstinline|eval_lvalue| that will give the same result as the
operational semantics relation.

Our typechecker is written in two layers: a translation from type
contexts and C expressions to \emph{syntactic} type-checking
assertions, and a denotation function from this syntax into Coq
propositions \lstinline|Prop|.  We can therefore
\emph{computationally} simplify assertions even before taking their
denotation.  Consider the expression \lstinline{3 + x}; the
precondition needed to evaluate this is 
\lstinline|True /\ initialized(_x)|. The \lstinline|True| is redundant, but if this
assertion were generated directly, no more computational progress can
be made. Instead our context $\Delta$ records that $x$ is an
initialized integer variable and computes the simplified syntactic
TC-assertion \lstinline|tc_TT|, whose denotation is \lstinline|True|.
This simplification improves performance even when using Ltac. We
further improve performance even in Ltac by using computation to
simplify our separation logic predicates.


\chapter{Canonical Forms for Assertions}

VST uses assertions to reason about programs, but what does 
it require about the form of these assertions? This chapter
discusses that, as well as how restricting the form of
assertions improve both the usability and the performance
of symbolic execution.

\section{Semi-Canonical Form}

We previously described \cite{appel14:plcc} assertions in the form
~~\lstinline{PROP $~P$ LOCAL $~Q$ SEP $~R$}.  Here each item in
\lstinline|$P$:list prop| is a pure assertion that doesn't refer to
the program state. \lstinline|$Q$: list (environ -> prop)| contains
assertions that can reason about local variables.  and 
\lstinline|$R$: list (environ -> mpred)| has spatial assertions that can reason about
both local variables and memory.  $\PROP$ folds conjunction over the
elements of $P$, $\LOCAL$ folds lifted conjunction ( 
\lstinline|`and : (environ -> Prop)-> (environ -> Prop) -> (environ -> Prop))|, and
$\SEP$ folds the separation logic \lstinline|*|, or separating
conjunction operator. $P$, $Q$, and $R$, are represented as lists
because, particularly in the case of $\SEP$, it is often convenient to
refer to the $n$th conjunct. This is much easier to implement when our
assertion is a list.  We say that assertions in this form are in
semi-canonical form.

At the lowest level, the Verifiable C logic rules are completely
unaware of any sort of canonical form. They generally refer to
assertions as single variables, using entailments to constrain them
rather than imposing syntactic requirements on them. The Floyd
automation system contains higher level lemmas that require assertions
to be in semi-canonical form, but also guarantee that the side
conditions that result from using the rules will be in semi-canonical
form.

\subsection{Substitution in semi-canonical form}

At the Verifiable C level, there is no choice but to use a semantic
notion of substitution called 
\lstinline|subst {A: Type} (x : ident) (v:val) (P : environ -> A) : environ -> A|.
This is because we know nothing about the assertion at all, only that
it takes an environment and returns an \lstinline|mpred|. The following example
shows how \lstinline|subst| is used:

\begin{lstlisting}
$\inference[semax\_set\_forward]{}{
\Delta\vdash\triple{\later P}{~x:=e~}{\exists v.\,x=(e[v/x])\wedge P[v/x]}
}$

Axiom semax_set_forward: $~~$forall $\Delta$ ($P$: environ->mpred) ($x$: ident) ($e$: expr),
  semax $\Delta$
    (|> (local (tc_expr $\Delta$ $e$) && local (tc_temp_id id (typeof $e$) $\Delta$ $e$) && $P$))
    (Sset $x$ $e$) 
    (normal_ret_assert 
      (EX old:val, local (`eq (eval_id $x$) (subst $x$ (`old) (eval_expr $e$)))
                    && subst $x$ (`old) $P$)).
\end{lstlisting}

There are two substitutions here, used to replace any occurrences of
the variable \lstinline|x| that might have occurred in either the
precondition or the expression being assigned into \lstinline|x|.
Although subst is a function, in practice it can never be computed.
This is because it works by updating the environment that $P$ refers
to. During symbolic execution, however, the environment is always
abstract, constrained only by the precondition, which means there is
no datastructure to be updated. This means that the definition of
\lstinline|subst| that appears in Verifiable C isn't directly useful
to proof automation. It can't compute so without special lemmas and
tactics it will appear in side conditions. To deal with this the Floyd
system has an autorewrite database that lets it push subst through
functions that won't be affected by the substitution. For example

\begin{lstlisting}
Lemma subst_sepcon: forall i v (P Q: environ->mpred),
  subst i v (P * Q) = (subst i v P * subst i v Q).
\end{lstlisting}

Fortunately, we don't need a lemma for every function that might
appear in assertions. Lifted functions can't do anything with the
environment, they can only pass it on to their arguments, so
by creating autorewrite rules for lifted functions we cover
most of the functions that we use, and also most functions
that a user might want to write. 

Semantic substitution is still inconvenient for a few reasons. First,
the rewrite rules aren't complete. This means that in some cases, after
applying a logic rule, the user will see a \lstinline|subst| in a
resulting condition. This can stop the automated entailment
solvers from working correctly and make the assertion much harder
to read. The next problem is an issue with autorewrite in general.
Autorewrite in Coq is slow. Rewrites aren't known for their 
performance, and autorewrite can do a large number of rewrites
(in the case of \lstinline|subst| the number of rewrites is
linear in the size of the assertion being rewritten). 

There is a situation when a substitution \lstinline|subst $x$ $v$ $P$| can
be avoided completely. That is when $P$ is \emph{closed} wrt. 
$x$, also a semantic notion:

\begin{lstlisting}
Definition closed_wrt_vars {B} (S: ident -> Prop) (F: environ -> B) : Prop := 
  forall rho te',  
     (forall i, S i \/ Map.get (te_of rho) i = Map.get te' i) ->
     F rho = F (mkEnviron (ge_of rho) (ve_of rho) te').
\end{lstlisting}

Generally we give \lstinline|S| as Coq equality with a specific
identifier.  What \lstinline|closed_wrt_vars| means, then, is that if
\lstinline|F| is supplied an environment that is the same at all
locations but the identifier(s) \lstinline|i| that satisfy
\lstinline|S|, the result of \lstinline|F| will be the same. That
means that if we know \lstinline|closed_wrt_vars (eq x) (e)|, we can
easily prove \lstinline|subst x _ e = e|. More intuitively, if an
expression doesn't contain a variable, a substitution on that variable
won't change the expression.

Floyd has a set lemma that takes advantage of this, stating that if
the precondition and the expression in the assignment are closed wrt
the variable being assigned into, no substitutions are needed, but
there are numerous cases where this rule doesn't apply, so the
substitution will still appear.

\section{Canonical Form}

The reason that substitutions are difficult, and that they need to be
semantic is because there is no \emph{syntactic} restriction on where
any individual identifier can appear within an assertion.  Canonical
form imposes such a restriction, and in doing so, eliminates the need
for semantic substitution, replacing it with a more efficient and
convenient computational syntactic substitution.

One limitation of canonical form is that we no longer allow references
to C program variables in the part of the assertion that contains
spatial assertions. If these assertions wish to talk about those
variables, they must do it indirectly using a Coq variable.  This
means that only the $\LOCAL$ part of the assertion has the ability to
reference local variables. This still doesn't give us the ability to
syntactically locate each reference though, so we restrict $\LOCAL$
further. The restriction we use is to change the entirety of $\LOCAL$
into two computational maps from identifiers to values.  One of these
represents temporary or nonadressable variables, and the other
represents addressable variables. Each mapping represents an equality
between the evaluation of an identifier in the environment, and the
value it maps to. The mappings are represented by PTrees, an efficient
computational data structure in the Coq standard library.

With these two changes we get \emph{canonical form.}  Let
\lstinline{$T_1$: PTree val} be a computational map from C program
identifiers to C values, representing the current values of the
temporary local variables of the current program state. Let
\lstinline{$T_2$: PTree (type*val)} be a map from identifiers to
\lstinline{type*val} representing the addresses of addressable local
variables.  Then \lstinline{localD $T_1$ $T_2$: list(environ->Prop)}
means a list of assertions about the contents of the
\lstinline{environ}, the nonmemory portion of the program state; we do
not need \emph{arbitrary} assertions of type
\lstinline{list(environ->Prop)}.

\lstinline{localD} is a \emph{denotation function}, reflecting the
syntactic (computationally oriented) $T_1$ and $T_2$ back into our
semantic world.  In symbolic execution and efficient entailment
solving, we operate directly on $T_1$ and $T_2$, reflecting the
results back only when the less efficient (but easier to understand)
semantic view is needed by the user. Now a full assertion is:

\begin{lstlisting}
assertD $P$ (localD $T_1$ $T_2$) $R$ : environ->mpred
$P$ : list prop$\qquad$ $T_1$ : PTree val$\qquad$ $T_2$ : PTree (type * val)$\qquad$ $R$ : list mpred
\end{lstlisting}

\subsection{Substitution in Canonical Form}

Substitution in this assertion is as simple as adding/replacing a
mapping in $T_1$ or $T_2$. To see why imagine that we are doing a
substitution on a temporary variable $x$.  \lstinline|$P$ : list prop|
and \lstinline|$R$ : list mpred| don't refer to an environment, so
they are trivially closed wrt.  $x$. This leaves $T_1$ and $T_2$. The
variable $x$ is a temporary variable, so we know $T_2$ is closed
wrt. $x$.  The map $T_1$, however, might have a reference to $x$,
meaning we actually need to do a substitution, making sure to replace
every reference to that variable. One of the requirements of a Coq map
is:

\begin{lstlisting}
Axiom PTree.gss
     : forall (A : Type) (i : positive) (x : A) (m : PTree.t A),
       PTree.get (PTree.set i x m) i = Some x
\end{lstlisting}

This means that if we update $x$ in some PTree, the old mapping of $x$
will no longer exist, which is the exact definition we want from a
substitution. That is how you do a substitution in an assertion, but
we still need to do substitution in the arbitrary C expression that
appears in the assignment statement and turn that expression into a
value. It is simple enough to turn a C expression into an
\lstinline|environ->val| using the \lstinline|eval_expr| function
discussed in \ref{}, but that expression could have references to
identifiers, which we can't have if we want syntactic
substitution. Instead we can write a different version of
\lstinline|eval_expr| called \lstinline|msubst_eval_expr|. The only
difference between the two functions is that when
\lstinline|msubst_eval_expr| needs to evaluate a variable it doesn't
do it in and environment. Instead, it performs the lookup in PTrees
$T_1$ or $T_2$ mentioned earlier. In other words, if
\lstinline|eval_expr| evaluates an expression in an environment,
\lstinline|msubst_eval_expr| symbolically evaluates an expression in
an assertion. This symbolic evaluation is partial because there might
not be any information about a variable in the assertion. So in our
lemma we require \lstinline|msubst_eval_expr| to succeed:

\begin{lstlisting}
Axiom semax_PTree_set: $~~$forall $\Delta$ id P T1 T2 R $e$ v,
  msubst_eval_expr T1 T2 $e$ = Some v ->
  semax $\Delta$
    (|> local (tc_expr $\Delta$ $e$) && local (tc_temp_id id (typeof e) $\Delta$ e) 
            && (assertD P (localD T1 T2) R))
    (Sset id $e$)
    (normal_ret_assert (assertD P (localD (PTree.set id v T1) T2) R)).
\end{lstlisting}

What that means is that for this lemma to be used, the precondition
must have mappings for every variable that appears in $e$. The
previous lemma didn't require this because it was able to use
\lstinline|eval_expr| wherever it wanted to without requiring a
complete, successful, symbolic execution. This still works for proofs
because \lstinline|`eval_expr $e$| might eventually simplify to
\lstinline|`eval_id $x$|, which could appear in other places in the
assertion.

\lstinline|semax_PTree_set| also doesn't have an existential. This
makes things simpler for the user and the proof automation. The
existential for the old value isn't terribly inconvenient on it's own,
it can be moved to the outside of the triple and introduced without
much difficulty, especially because it's location in the precondition
is consistent. The difficulty comes when choosing what to name the
introduced variable. The solution in the tactics is to allow the user
to specify names with the name tactic. Using \lstinline|name y _y|)
tells the automation that values associated with variable
\lstinline|_y| should automatically be named \lstinline|y| or
\lstinline|y0|, \lstinline|y1|, \ldots if it isn't available.  This is
a decent solution but it puts a hypothesis above the line, adding to
what can already be a long list of hypotheses. It is also inconvenient
in programs that make multiple assignments into the same variable. The
following program is an example of what you might see without improved
tactics or user cleanup:

\begin{lstlisting}
{`eq (eval_id _x) x}
x = x+1;
{`eq (eval_id _x) (x0 + 1); `eq (x0 x)}
x = x+2;
{`eq (eval_id _x) (x1 + 1); `eq x1 (x0 + 1); `eq x0 x}
...
\end{lstlisting}



Semi-canonical form is very convenient for the user when
\emph{writing} assertions. The list notation is great for combining
assertions without having to remember the exact conjunction that must
be used for each part. It also allows the $\LOCAL$ to remain small,
because if there is a variable the user knows nothing about, there is
no need to add it to the locals. It is less convenient when moving
through a proof of a program. It can introduce existentials and
substitutions that are slow to simplify, or sometimes don't simplify
at all.

Even the current form is not completely canonical. It could be
restricted further which would improve performance in some ways, but
also inconvenience the user in others. Finding a way to sort the
$\SEP$ and keep it sorted as new conjunctions are added could lead to
very efficient and simple entailment solving. This is a harder problem
than sorting the $\LOCAL$ though, because we want $\SEP$ to contain a
variety of predicates, including predicates that are created by the
user. The other problem is that Coq variables can appear as arguments
to the predicates, making it impossible to establish an ordering over
them. Even so, any amount of canonicalization of the $\SEP$ could help
with entailment solving efficiency, so it is worth a look in the
future.

\chapter{Applying A Reflective Framework}

\section{Modular Reflection}

Before presenting the reflective framework we presented both the
typechecker, and a canonical form for assertions which allows us to do
substitutions computationally.  Both our techniques and the reflective
framework are reflective.  They use proved-sound Coq functions to make
progress in a proof. This section discusses how these techniques
interact. There is no interesting interaction between the substitution
and the typechecker because they occur in different places. When we
try to apply a reflective framework to a logic that uses these
reflective techniques, though, we run into some interesting
challenges. This section discusses those challenges and questions if
they are avoidable in a differently designed reflective framework.

A function that is used in a logic can fall into one of three
categories with respect to their interaction with the reflective
framework:

\begin{itemize}
  \item A function that operates on constants and returns a type that can be represented as a constant
    requires no modification to be used in MirrorCore.
  \item A function that operates on constants and returns Prop or Type will look exactly
    the same but return reified results.
  \item A function that might operate on Coq variables must take reified expressions as 
    arguments and return a reified result.
\end{itemize}

The first category is convenient, but unfortunately fairly few functions that
we use fall into this category. The reason that they can return any
type but Type or Prop is because in general if a function returns a result we
will want to reason about it, and if it can't be represented by a constant
we will be unable to do anything with it in the Mirror-Core framework.

The second category includes the typechecker which returns
\lstinline|environ -> Prop| that might eventually be discharged
by either the person writing the proof or some automation. 
In order to allow reflective automation to have a chance to solve these remaining
conditions, we must write a new function who instead of returning
\lstinline|environ -> Prop| returns an \lstinline|expr| whose denotation
is (provably) the same as the result of the original typechecker called
on the same arguments.

\begin{lstlisting}
Definition tc_expr_reif (e : c_expr) (Delta : tycontext) : expr.

Lemma tc_expr_reif_sound_complete : forall tus tvs e Delta,
exprD' tus tvs (tyarr tyenviron typrop) (tc_expr_reif e Delta) =
exprD' tus tbs (tyarr tyenviron typrop) (ftc_expr e Delta).
\end{lstlisting}

Notice that the right hand side of the equality in the lemma doesn't
refer to the original typechecking function but the trivially reified
version. That is, \lstinline|ftc_expr| is a constructor whose
denotation is equal to \lstinline|tc_expr|, while
\lstinline|tc_expr_reif| is a function that when applied will have a
denotation equal to \lstinline|tc_expr| applied to the same
arguments. The function \lstinline|tc_expr_reif| can compute while no
progress can be made on \lstinline|ftc_expr|.  We put the denotation
function on both sides of the function because this allows us to write
an RTac that finds reified terms that look like \lstinline|ftc_expr e Delta|
(uncomputable) and replace them with a term 
\lstinline|tc_expr e Delta|. This is important because we might want to run some other
RTac on the result of the typechecking. Possibly an entailment solver
to discharge any remaining conditions.

We prove that \lstinline|tc_expr_reif| is sound, or that when it has a
denotation, that denotation matches the \lstinline|tc_expr|
function. We also prove it complete by showing that whenever
\lstinline|ftc_expr| has a denotation, \lstinline|tc_expr_reif| does
as well.  In this case sound and complete is the most useful so we
prove it. We have found other cases where only soundness is necessary,
and will discuss those later.

The final category is functions whose inputs might contain Coq
variables. An example of where we run into this is the
\lstinline|PTree.set| and \lstinline|PTree.get| operations that are in
the postcondition of our assignment rules. These functions can be
computed on their own, even if they are given a \lstinline|PTree| with
a coq variable in it as an argument. The problem is that there is no
way to represent any datastructure that contains a coq variable as a
constant. This is because there is no way to computationally compare
two Coq variables for equality. In order to computationally check
equality over Coq variables, they must first be reified.  Reification
is designed so that if it sees the same variable twice, it will use
the same constructor to represent both instances. The constructors can
be computationally compared, so variables can be compared in reified
syntax.  We will need to do comparisons on the variables in the local
assertion almost any time we solve an entailment, so we will need to
reify the variables.

What that means is that we must rewrite \lstinline|PTree.set| and
\lstinline|PTree.get| to operate on fully reified PTrees. This can be
difficult because of the size of the reified syntax. One way to
mitigate the complexity of the code is to create a function that
matches a reified expression as a tree:

\begin{lstlisting}
Definition as_tree (e : expr typ func) : option
  ((typ * expr typ func * expr typ func * expr typ func) + typ) := 
match e with
  | (App (App (App (Inj (inr (Data (fnode t)))) l) o) r) =>
    Some (inl (t, l, o, r))
  | (Inj (inr (Data (fleaf t)))) =>
    Some (inr t)
  | _ => None
end.
\end{lstlisting}

This allows the function that operates over reified expressions to look very
similar to original expression, and also simplifies the proof. The function
and the proof will now have 3 cases. The first have the same behavior (only reified)
as the \lstinline|leaf| and \lstinline|node| cases of the original function. The third
case is the case where \lstinline|as_tree| returns \lstinline|None|. This doesn't
mean that the function doesn't typecheck because there is no requirement on 
expressions that a PTree can only be represented by those two constructors. There
could be another constructor with the same denotation, or more likely a PTree
could be an application of \lstinline|PTree.set| that didn't get simplified.
If we encounter a reified PTree that isn't represented
only by the expected constructors, we have a choice depending on
weather we require completeness or not. If we require completeness,
we can just apply the set function as a reified constructor. This
will result in a complete function. That means that if we
pass our reified function all valid reified arguments
the denotation of the result of the function will always
match the denotation of the original function applied
to the denotation of the arguments. It is the most convenient
to write an RTac for reified functions of this type because
there is a pre-built RTac that replaces one reified expression
with another. This RTac can only be proved sound if there is a proof
of equality between any two expressions that the tactic might come
across. The only downside to this approach is that proving that the
reified function is complete takes much more work than only proving it
sound. 

These reified functions are an inconvenience and one of the largest
barriers to easily applying a reflective framework in a setting like
VST. It is an important open question weather this is avoidable. Can
a reflective framework reuse computations that might operate
over Coq variables without requiring reified functions? 

\chapter{Evaluation}

Mostly same as paper evaluation, more discussion of why the synthetic
example is a useful metric for what we can expect in real programs, 
maybe a synthetic example for load/store too

\chapter{Conclusion}

\bibliographystyle{plain}
\bibliography{appel.bib}

\end{document}

