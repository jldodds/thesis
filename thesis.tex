\documentclass{puthesis}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{url}       % SB
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{times}
\usepackage{xcolor}
\usepackage{textcomp}
\usepackage{mathpartir}
\usepackage{semantic}
\usepackage{listings}
\usepackage{lstlangcoq}
\usepackage{caption}
\usepackage{stmaryrd}
\usepackage{minted}
\usepackage{placeins}
\renewcommand{\lstlistingname}{Figure}
\input{macros}



\lstset{language=Coq,alsoletter=_,basicstyle=\sffamily,mathescape=true,columns=fullflexible}



\author{Josiah Dodds}
\adviser{Andrew Appel}
\title{Computation Improves Interactive Symbolic Execution}
\abstract{The abstract goes here.}
\acknowledgements{Thank you very much.}
\dedication{To myself.}



\begin{document}



\chapter{Introduction}

The C programming language is one of the most commonly used languages
for writing critical software. C is often used for operating systems,
embedded systems, and cryptography, as well as countless libraries
that need to have a great performance while being usable by as many
languages and systes as possible. The world depends on more systems
every day, and a large number of those systems have C code at their core.

In general, the code we depend on works well, and there is plenty of
code that can fail without causing harm but when it fails it can be
catastrophic. Space projects that cost hundreds of millions of dollars
have failed due to software failures, radiation devices with software
flaws have overdosed cancer patients, nuclear missile detection
systems have given false positives, and millions of cars have been
recalled to fix software bugs. Those scenarios are only bugs; Far more
systems can fail when a malicious user attacks them. As an example
a brand of refrigerators was found to have been taken over to send
spam email. 

To believe that a C program will run correctly we must first believe a
number of other things. There are a number of components that we will
need to be able to \emph{trust} in order to trust our C program. We need
to fully understand the program itself, and believe that it will do
what we want it to. We also need to believe that when
the C code is translated into machine language, the generated machine
language has the same behavior as the original program. Finally we
need to believe that the machine executing the machine language is
doing the correct thing. We call the collection of things we must
trust the \emph{trusted computing base}. In general, the larger the
trusted computing base is, the harder it is to be confident that the
program is correct.

The need for trust follows a chain. It starts with the C program at
the top, the most human understandable part of the chain, and moves
down to the actual execution of the machine. In order to make the
trusted computing base as small as possible, we must maximize our
confidence in every step of the process.

Proof assistants are tools for stating theorems, and then writing and
checking proofs of those theorems. If you want to believe a pen and
paper mathematical proof you first need to understand the statement of
the proof, which is open to ambiguities based on (often unstated)
assumptions made by the writer, and then believe every single step of
the proof itself, even steps where the proof author says something
vague like ``clearly'' or ``obviously''. This can be an enormous
burden on the person that needs to understand how the proof was done;
new proofs can use new techniques that are impenetrable to anyone but
experts in the specific domain of the proof. This is a real problem,
because often the thing being proved is incredibly useful to a large
range of people even if the proof itself is difficult for them to
understand. If you want to use a pen and paper proof that you can't
understand, you must trust the process of peer review. Maybe you
noticed that the paper appeared in a good publication. This means that
some number of anonymous reviewers believed the proof. They are
probably experts, and you want the proof to be true anyways so you can
use it, so you go ahead and use their theorem for your own work.

A proof assistant, on the other hand, forces proof statements to be
written using a set of basic, well defined building blocks, making
theorem statements completely unambiguous. The proof assistant often
provides tools for building proofs. These tools write proofs using a
similar set of well-defined building blocks to the ones used for
stating the theorem. Again, these building blocks are
unambiguous. There is not building block for ``clearly'' or
``obviously'', all proofs must be fully explicit all the way down.
This means that the tools for building proofs don't need to be
trusted. All that needs to be trusted is a part of the proof assistant
that checks the proof when it is created. This is analogous to not
needing to trust the person writing a pen and paper proof, as long as
you trust the proof that they wrote. That means that the proof itself
is irrelevant to someone interested in believing the theorem. All
that is relevant is that the proof assistant successfully checked the
proof, and that the proof assistant's checker is correct. This means
that if a proof is stated and proved in a proof assistant, to believe
the proof we must trust:

\begin{enumerate}
  \item The statement of the theorem being proved, and
  \item the correctness of the checker. 
\end{enumerate}

Because the writers of proof assistants are aware of the importance of
having a small trusted computing base, they often make an effort to
make the checker as simple and minimal as possible. As small as the
checker is, it can still be difficult for an average user of the proof
assistant to believe on their own. Once again, the best place to look
in order to believe the proofs in to peer review. In this case though,
it is not a few anonymous reviewers that are looking at an individual
proof, it is the entire community that is interested in the
correctness of the proof assistant. This is an advantage of
concentrating trust. The fewer things that need to be trusted, the
more attention they can get.


A verified compiler gives a proof that if it is given a valid C
program, it will generate assembly code with the same behavior as the
C program. This proof is checked by the Coq proof assistant, meaning
that its trusted base is the Coq proof checker and the statement of
the theorem. To be able to use the theorem effectively though, we need
to be able to show that the C program will always have a valid
execution. 

The results of the verified compiler tells us that the compiler won't
change the behavior of the program, but this is only really useful to
us if our program is doing the right thing in the first place. It can
be very hard to determine what a C program is doing just by examining
the source code.  A typical C program doesn't just declare what it is
doing, it declares how it is doing it. The C language is relatively
low level meaning that a program will need more details about how the
computation is done than a higher level language. The high level of
control over the details of execution is a large part of why C
continues to be popular over 40 years after it was created. It allows
programmers to write more efficient programs, but it can make it much
harder to understand what the program is actually doing.

In order to specify what a program is doing rather than how it is
doing it we write a mathematical specification of what the program
should do. This mathematical model lacks the complexity that a real
program needs to work efficiently on a computer.  The specification is
a new link in the chain above the C source code, but I have not
described how it is attached to the rest of the chain yet. A
\emph{Program Logic} is used to prove that the result of executing the
C program meets the specification. This program logic can be written
in Coq, and proved correct using the same specification of the C
language that CompCert uses. With this proof, the chain can be
completed from specification all the way to assembly language.

The combination of the tools mentioned here gives proof that the
assembly code that the machine executes has the same behavior as an
abstract mathematical specification. To believe that the proof is
correct, all we need to trust is the mathematical specification itself
and the theorem prover's proof checker.

There is still a significant amount of work to do if you wish to prove
a program correct. First you must create an accurate specification
which can be difficult, especially for complex programs. Then the
process of proving a program meets a specification using a program
logic can be very involved. Program logics implemented in proof
assistants are often \emph{interactive}. To build a proof in an
interactive program logic, you examine a proof state and then perform
an action to manipulate that state. The action will result in a new
proof state, which you can repeatedly manipulate 
until the proof is completed.

One of the biggest problems that complex program logics run into is
that they are slow. This means that the time between giving input and
receiving a new proof state can be long. Writing a proof in a program
logic is already difficult work requiring intimate knowledge of the
program, the programming language, and the logic. Having to wait
between each step in a proof multiplies the difficulty.  This is
because proof development is often experimental in nature. The first
step is to make a good guess at a specification. If the proof gets
stuck, either the specification or the program needs to change. When
those change, the proof will need to start over from the beginning. A
slow program logic can drastically limit the number of manipulations
of the program or the specification that can be done in any given
stretch of time. If the time is long enough, it can become difficult
the person doing the proof to remember what they were working on when
they last made progress on the proof.

This thesis shows how a complex programming logic for proving the
correctness of C programs can be made significantly faster without any
noticeable impact on the proving experience. This results in a logic
that is significantly more intuitive and usable, and is a vital step
in making the program logic a widely usable tool.

Many of the parts discussed so far are existing work: The proof
assistant we have chosen to use is Coq, Leroy's CompCert \cite{} is a
C compiler implemented and proved sound in Coq, and Appel et al. have
created the Verified Software Toolchain (VST) to give a proved-correct
chain from specification language to assembly including a
specification logic and a program logic to relate the specification to
a CompCert C program.

\paragraph{Contribution}
This thesis discusses the modification of a program logic to improve
usability and speed up the application of the logic. It consists of a
number of modifications to the logic and results in a tactic for
manipulating proofs about C programs. The tactic runs at least 40x
faster than previous tactics and works on all verifiable-c non-call
assignment statements (\lstinline|x := e|), meaning it makes progress
on a single basic block at a time. The tactic is fully compatible with
previous tactics, meaning that in places where it is not usable,
existing tactics can still make progress on the proof.

\chapter{Computation in Coq}
\label{ch:computation}
The Coq proof assistant has a built in functional programming language
called Gallina with the ability to match inductive data-structures. The type
boolean, for example can be defined:

\begin{minted}{coq}
Inductive boolean :=
| true
| false.
\end{minted}

so \lstinline|boolean| is a type that can be constructed by either
\lstinline|true| or \lstinline|false| and when given a
\lstinline|boolean|, a function can decide which of the two it is. Now
we can look at the definition of \lstinline|True| and
\lstinline|False|, which are in the type
\lstinline|Prop| instead of \lstinline|boolean|. Prop is not an
inductively defined type in Coq, so it can't be matched
against. Instead the definitions of \lstinline|True| and \lstinline|False|
are

\begin{minted}{coq}
Inductive True : Prop := I : True.
\end{minted}
\begin{minted}{coq}
Inductive False : Prop.
\end{minted}

\noindent 
Gallina programs cannot pattern-match over Propositions; they can
compute only over inductive data structures, and Prop is not
inductive.  However, the tactic language can parse (pattern-match on)
propositions.  so if a Gallina program is given something of type
\lstinline|Prop|, it has no ability to determine more information
about it, just like if it is given a polymorphic value it is unable to
determine what the type of the value is is. That doesn't mean that it
is impossible to reason about things of type \lstinline|Prop| in
Coq. Instead of using Gallina to operate on \lstinline|Prop|, Coq has
tactics, which exist almost exclusively for that reason. A tactic in
Coq is a program that manipulates a proof state while generating a
proof object recording its activity. A Coq proof state is a goal, or a
\lstinline|Prop| to be proved, along with a number of quantified
variables of any type, and a number of hypothesis of type
\lstinline|Prop|. Because tactics are used to build proofs, it must
keep a record any time it makes progress on the proof term so that
when the proof is over, the record of the proof, or the proof object,
can be checked.

This design means that tactics don't need to be sound. As long as they
result in a correct proof in the end, it doesn't really matter how
they did it. Proofs that do a large amount of proof search to do a
reasonably small amount of work in the end can perform reasonably well
in tactical proof. Unfortunately, many of the proofs that Coq's users
want to do require both a lot of proof search, and numerous operations
over the proof state to succeed. In these cases, the overhead of
keeping and modifying the proof object (which can take up substantial
amounts of memory) can lead to very slow tactic performance.

Logics such as VST's verifiable C program logic are in
\lstinline|Prop|. Logics in Prop are said to be shallowly embedded.
Instead of a "deep embedding"--syntax separated from any semantic
interpretation--a shallow embedding defines each new operator as a
semantic predicate in the language of propositions.  Some systems
such as Appel's VeriSmall \cite{} are deeply embedded, meaning they
define their own syntax, along with a denotation that gives meaning to
that syntax in Coq's logic.

Shallow and Deep embeddings have tradeoffs in two areas:

\begin{enumerate}
\item interactivity, or how convenient it is for users to interact
  with the logic, generally in the way they would expect to be able to
  interact with Coq's logic, and
\item automation, or the ability of the logic writer to provide the
  user of the logic with tools to efficiently reason about the logic.
\end{enumerate}

For interactivity, a shallow embedding will automatically get all of
the automation that Coq provides. Ltac can be used to write decision
procedures about the logic without requiring soundness proofs for
those procedures. A deep embedding, however, will be much harder to
interact with. To have meaning as a proof goal, a deep embedding will
need to be wrapped in its denotation function. Then every operation on
the deeply embedded assertion must be proved sound with respect to the
denotation function. Take Coq's rewrite tactic as an example. In this
tactic if we have a lemma about an equality:

\begin{minted}{coq}
Lemma add_symm : forall a b, a + b = b + a
\end{minted}

and we wish to prove a goal 

\begin{minted}{coq}
a : nat
b : nat
c : nat
======================
a + (b + c) = a + (c + b) 
\end{minted}

we can use the tactic \lstinline|rewrite (add_sym b c); reflexivity|
to transform the left side to match the right side, and then tell Coq
that we have an equation where the two sides are syntactically
equal. We supply the arguments \lstinline|b| and \lstinline|c| to the
tactic because it is ambiguous where to rewrite the lemma, and Coq
might guess wrong unless we tell it.

Now we can create a deep embedding for nat, addition, and equality

\begin{minted}{coq}
Inductive expr' :=
| num : nat -> expr
| add : expr -> expr -> expr.

Inductive expr :=
| eq : expr' -> expr' -> expr.

Fixpoint expr'_denote e:=
match e with
| num n => n
| add e1 e2 => expr_denote e1 + expr_denote e2
end.

Definition expr_denote e :=
match e with
| eq e1 e2 => expr'_denote e1 = expr'_denote e2
end.
\end{minted} 

Then we can write a symmetry lemma

\begin{minted}{coq}
Lemma add_sym' : forall a b, 
expr'_denote (add a b) = expr'_denote (add b a)
\end{minted}

While this isn't too hard to prove, it also doesn't end up being too
useful in proving our goal:

\begin{minted}{coq}
a : expr
b : expr
c : expr
======================
expr_denote (add a (add b c)) (add a (add c b))  
\end{minted}

The lemma we wrote doesn't match the syntax of what we need to prove
so we can't use the rewrite tactic. We could unfold the definition of
\lstinline|expr_denote| and \lstinline|expr_denote'| in our goal, but
then we would lose the deep embedding. Instead, if we want this
functionality, we need to write a \emph{Coq function} that does the
rewrite and prove that function sound with respect to the denotation
function (assuming we have an equality function on our expr, which
isn't hard to write):

\begin{minted}{coq}
Fixpoint symmetry' e e1 e2 := 
match e with
| add e1' e2' => if (e1 == e1' && e2 = e2') 
                 then add e2' e1' 
                 else add (symmetry e1') 
                          (symmetry e2')
| x => x
end.

Definition symmetry e e1 e2 :=
match e with
| eq ex1 ex2 => eq (symmetry' ex1 e1 e2) (symmetry ex2 e1 e2)
end.

Lemma symmetry_func_sound : 
forall e1 e2 e,
expr_denote (e) = expr_denote (symmetry e e1 e2)
...
\end{minted}

Now we can rewrite by \lstinline|symmetry_func_sound| and simplify the definition
of symmetry in the new goal, and we will have done the same thing we
did with LTac in a single command.

Why would we ever use a deep embedding when automation is so much
work? The main reason is efficiency. While LTac does an operation and
needs to build a proof object, a proved-sound function that operates on
a deeply embedded proof goal does an operation and at the same time is
a (very small) proof object. The example above did a very small amount
of work in the symmetry function, but such a function could be an
entire decision procedure. Then we would be able to do a large
amount of work on a deeply embedded proof term while hardly generating
any proof object at all. In general, this will be substantially more
efficient than using LTac, especially as the size of the
deeply-embedded statements grows. 

Another advantage to a deeply embedded logic is that if there is a
single decision procedure for it that works all in one function, that
function can be \emph{extracted} and run in OCaml. This gives the
performance of an optimized language, with most of the assurance that
comes with having a proved-sound decision procedure in Coq. Even if
there is not a decision procedure, parts of the logic can be executed
efficiently in Coq using the \lstinline|vm_compute| tactic, which
compiles Coq to bytecode that can be executed in a virtual machine. 

The technique of computational reflection in Coq aims to combine the
interactivity of a shallow embedding with the efficiency of a deep
embedding by moving between the two when it is needed.

\section{Computational reflection}
\label{sec:reflection}

Proof by reflection is the technique of using proved-sound computation
on a deep embedding to make progress in a proof. To use reflection on
a shallow embedding, there will first need to be a translation from
the shallow embedding into a deep embedding. Then a proved-sound
function can be evaluated on the deep embedding, and if it is
appropriate the denotation function can be evaluated, resulting in a
shallow embedding again.  If this is done all in a single step, the
user of a reflective tactic never needs to know that reflection is
being used. That means that the efficient, proved-sound decision
procedures that can be used on deep embeddings can essentially be used
as if they are tactics, along side all of the other tactics and LTac
automation that might be built up around a logic.

The process of translating from a shallow embedding to a deep
embedding is known as reification. Because Galina programs can't match on
shallow embeddings, reification must be performed by a
tactic. Reification leaves us with a deep embedding (or reified
expression), which allows us to use proved-sound Gallina functions to
progress the proof state without large proof terms. When the deep
embedding is at a state that requires human work, the denotation
function can be carefully unfolded, or reflected, to return it to a
shallow embedding that is easy for the user to view and work with.

%graphic from prefpo 

Deep embeddings are purely syntactic; but even shallowly embedded
languages may have substantial sublanguages that are purely
syntactic--defined by inductive datatypes rather than semantic
definitions. In many program logics, for example, the programming
language will not be a denotational semantics based on Gallina, but an
operational semantics over syntactic program terms. Because the syntax
is a deep embedding, it is possible to write Coq functions that reason
about them, and then prove those functions sound (usually with respect
to the operational semantics). This is a type of reflection that
doesn't require reification at all, making it more efficient and
allowing it to fit cleanly into the shallowly embedded program
logic. One such example in VST is the typechecker which is used to
show that expressions in the program successfully evaluate given the
state represented by the precondition.


\chapter{Typechecking C Expressions}

A type system is a formalization that assigns types to various parts
of a programming language and describes when a programming language
uses those parts correctly according to their types.  It is often
possible to build a decision procedure that answers weather or not a
program is well typed in a type system. This decision procedure is
known as a typechecker.  A typechecker is generally meant to be used
as part of program compilation which means it should be reasonably
fast and operate with the program as it's sole input. It restricts the
number of valid programs while guaranteeing certain errors won't
occur.

We are not interested in a typechecker that works on an entire program
at a time but one that works on a single expression or lvalue. It is
important to us that no errors can occur in the evaluation of this
expression. To see why, consider a naive Hoare assignment rule for the
C language.
\begin{mathpar}
\inferrule{}
{\tripleD{}{P[e/x]}{x:=e}{P}}\qquad\mbox{(naive-assignment)}
\end{mathpar}
\FloatBarrier

This rule is not sound with respect to the operational semantics of
C. We need a proof that $e$ evaluates to a value. It could, for
example, be a division by zero, in which case the program would crash
and Hoare triples would not hold. The expression $e$ might
typecheck in the C compiler, but can still get stuck in the
operational semantics

A better assignment rule requires $e$ to evaluate:


\vspace{-20pt}
\begin{mathpar}
\inferrule{\exists v.  e \Downarrow v}
{\tripleD{}{P[v/x]}{x:=e}{P}}\mbox{assignment-ex}
\end{mathpar}
\FloatBarrier

The proof of this rule is relatively easy, but the rule is inconvenient to
apply because we must use the operational semantics to show that $v$
exists. In fact, any time that we wish to talk about the value that
results from the evaluation of an expression, we must add an existential
quantifier to our assertion. Showing that an expression evaluates can require a
number of additional proofs. If our expression is \lstinline|(y / z)|,
we will need to show that our precondition implies: \lstinline|y| and
\lstinline|z| are both initialized, \lstinline|z $\not=$ 0|, and 
\lstinline| $\texttildelow$(y = int_min $\wedge$ z =$\,$-1)|.
The latter case causes overflow, which is undefined in the C
standard. These requirements will become apparent as we apply the semantic
rules, and many of them will be trivially discharged. Even so, the
proof will be required, when it was likely computationally obvious
that it was unnecessary. A type system is the answer to this problem,
but unfortunately C has a mostly sound type system, when we need true soundness.


\section{Type soundness of C}

A type system is sound with respect to an operational semantics if a
well typed program will never go wrong. That can mean a variety of
things depending on the goal of the type system. Going wrong could be
a runtime error caused by a failed type conversion, dereferencing a
null pointer, or entering a state not described by the semantics of
the language. A type checker is typically a decision procedure that
decides if a program is well-typed in a type system. A trivially sound
type system and type checker reject all programs.

It is possible to create a decidable type system for C that accepts a reasonable
number of programs and is sound with respect to
a limited notion of ``going wrong''. It is impossible, however, to extend this
notation to any degree of memory safety, or more usefully to extend it
to state that a program will always exhibit defined behavior because
dynamic properties of the program can decide weather behavior is
defined or not. Examples are division by zero and dereferencing a null
pointer. If we consider programs such as

\begin{minted}{c}
if(undedicable thing){ x=0; }
3/x;
\end{minted}

it is clear that we can't hope to prevent such behavior with a
decision procedure. The best we could do would be to
disallow operations that might cause undefined behavior, but that
would leave us with an almost completely useless language with which
to write typechecking programs. 

Another option for creating a sound
type system for C would be to not consider such errors ``going wrong''.
The way that C defines these errors makes this impossible as
well. A program with division by 0 will generally crash, but this is
an expectation given by experience with a selection of C compilers, not a requirement of the C
programming language. Almost all C
programs that crash are actually exhibiting undefined behavior. That means that
the compiler can generate whatever code it wants and still be a valid
C compiler. For an example, a valid but unlikabe C compiler could
generate code that erases your hard drive in place of code that
performs division by 0.

This was done for performance reasons. It is nice for compiler writers
to not have to worry about malformed programs and instead write
optimizations and instructions that assume that programs will behave
nicely.  As a result, individual compilers generally exhibit
reasonably consistent behavior, but there is no real requirement for
them to do so. Changing compilers or even compiler settings could lead
to massive changes in the execution of programs that perform undefined
operations. This is especially dangerous because unless a programmer
pays careful attention to the C standard (a document with over 500
pages, 13 of which are used just listing the various undefined
behaviors), they might never know that their code is exhibiting
undefined behavior until it causes a problem.

The complete lack of semantics for situations like divison by 0 or
null pointer dereferences gives good performance, but it also means
that they must be considered ``going wrong'' because there is no way
to describe them as expected program behavior. If the language
definition and semantics said ``return a runtime error and halt on
division by 0'' such a definition might be possible, but instead the C
standard says ``do whatever you want on division by 0'' which is what
a compiler writer wants, but much useful to a programmer interested in
correctness.

Because the existing type system for C allows behaviors that are
certainly considered to be going wrong, we say that the C type system
is \emph{mostly sound}. It stops a number of bad behaviors and reduces
the number of runtime errors that a program will have, but doesn't
catch all of them. The C type system is \emph{mostly sound}, because
it would be impossible to create a decidable typechecker for a sound
type system. To get around this, we use a type system that is not
decidable, leaving the really difficult (or impossible) parts up to a
human.


\section{A Sound C Type System}

Instead we create a sound type system. We know that there is no way to
create a decidable type-checker for such a type system without
disallowing a substantial amount of the C language. Fortunately, we
don't need a decidable typechecker because it is used in the context
of an interactively applied program logic.  Instead, we build an
undecidable typechecker that works on a single expression at a time,
leaving the hard parts to a human by generating simple, minimal
preconditions to ensure expression evaluation. We define a function
\lstinline|typecheck_expr| (Section \ref{sec:typechecker}) to tell us
when expressions evaluate.  Now our assignment rules are,

\vspace{-20pt}
\begin{mathpar}
\inferrule{ }
{\tripleD{\Delta}{\mathsf{typecheck\_expr}(e,\Delta)~\wedge~P[e/x]}{x:=e}{P}}\mbox{tc-assignment}
\and
\inferrule{ }
{\tripleD{\Delta}{\mathsf{typecheck\_expr}(e,\Delta)~\wedge~P}{x:=e}{\exists
v. x = \mathsf{eval}(e[v/x]) \wedge P[v/x]}}\mbox{tc-floyd-assignment}
\end{mathpar}
\FloatBarrier

The $\mathsf{typecheck\_expr}$ is not a side condition, as
it is not simply a proposition (\lstinline|Prop| in Coq)
but a separation-logic predicate quantified over an environment (Chapter \ref{ch:canonical}).  
To make the rule usable we derive a new rule using the rule of
consequence:

\vspace{-20pt}
\begin{mathpar}
\inferrule{P \longrightarrow \mathsf{typecheck\_expr(e,\Delta)}}
{\tripleD{\Delta}{P}{x:=e}{\exists
v. x = \mathsf{eval}(e[v/x]) \wedge P[v/x]}}\mbox{tc-floyd-assignment}
\end{mathpar}
\FloatBarrier

To apply the rule you must show that the precondition implies
whatever condition the type checker generates. In this way, our
typechecker allows outside knowledge about the program, including
assumptions about the preconditions to the function to be used to
decide if an expression evaluates. 
For example, when run on
the expression $(y/z)$ the typechecker computes to the assertion $z \not= 0 
\wedge \texttildelow(y \not= \mathsf{int\_min} \vee
z \not= -1)$ where $z$ and $y$ are not the variables, but the values that
result when $z$ and $y$ are evaluated in some environment. The assertions
$\mathsf{initialized}(y)$ and $\mathsf{initialized}(z)$ may not be produced as
proof obligations if the type-and-initialization context $\Delta$ assures that
$y$ and $z$ are initialized.
The calculation of $\Delta$ is also part of our type system.

%TODO maybe this should be a section/chapter like it is in the reflection paper
We use the Floyd-style forward assignment rule,
instead of the Hoare-style weakest-precondition rule.
This is not related to type-checking; separation logic 
with backward verification-condition generation gives us magic wands which are
best avoided when possible \cite{berdine05:symbolic}.

In tc\_floyd\_assignment, we use a function $\mathsf{eval}$ which is a
function that has a similar purpose to compcert's
\lstinline|eval_expr| relation.  Defining evaluation as a function in
this manner makes proofs more computational---more efficient to build
and check.

We simplify \lstinline|eval_expr| in our program logic---and make it
computational---by leveraging the typechecker's guarantee that
evaluation will not fail.  Our total recursive function
\lstinline|eval_expr (e: expr) (rho: environ)|: in environment $\rho$,
expression $e$ evaluates to the value 
(\lstinline|eval_expr $e$ $\rho$|).  When \lstinline{CompCert.eval_expr} fails, our own
\lstinline{eval_expr} (though it is a total function) can return an
arbitrary value.  We can do this because the function will be run on a
program that typechecks---the failure is unreachable in practice. We
then prove the relationship between the two definitions of evaluation
on expressions that typecheck (we state the theorem in English and in
Coq):
\newtheorem{eval_expr_relate}{Theorem}
\begin{eval_expr_relate}
For all logical environments $\rho$ that are well typed with respect to a type
context $\Delta$, if an expression $e$ typechecks with respect to $\Delta$, the
CompCert evaluation relation relates $e$ to the result of the computational
expression evaluation of $e$ in $\rho$. 
\end{eval_expr_relate}

\begin{lstlisting}
Lemma eval_expr_relate :
$\forall$ $\Delta$ $\rho$ e m ge ve te,  typecheck_environ $\Delta$ $\rho$ -> mkEnviron ge ve te = $\rho$ ->
  denote_tc_assert (typecheck_expr $\Delta$ e) $\rho$ ->
  Clight.eval_expr ge ve te m e  (eval_expr e $\rho$)
\end{lstlisting}

Expression evaluation requires an environment, but when writing
assertions for a Hoare logic, we actually write assertions that are
functions from environments to \lstinline|Prop|. So if we wish to say
``the expression $e$ evaluates to $5$'', we write 
\lstinline|fun $\rho$ => eq (eval_expr e $\rho$) 5|.  Because Coq does not match or
rewrite under lambda (\lstinline|fun|), assertions of this form hinder
proof automation.  Our solution is to follow Bengtson \emph{et al.}
\cite{bengtson12} in \emph{lifting} \lstinline|eq| over $\rho$:
\lstinline|`eq (eval_expr e) `5|.  This produces an equivalent
assertion, but one that we are able to rewrite and match against. The
first backtick lifts \lstinline|eq| from \lstinline{val->val->Prop} to
\lstinline{(environ->val)->(environ->val)->Prop}, and the second
backtick lifts \lstinline{5} from \lstinline{val} to a constant
function in \lstinline{environ->val}.


\section{C light}
\label{sec:clight}
Our program logic is for C, but the C programming language has features that are
unfriendly to Hoare logic: \emph{side effects within subexpressions} 
make it impossible to simply talk about ``the value of $e$''  and \emph{taking
the address of a local variable} means that one cannot reason straightforwardly about
substituting for a program variable
(as there might be aliasing).

The first passes of CompCert translate
\emph{CompCert C} (a refined and formalized version of C90
\cite{leroy13:compcert}) into \emph{C light}.
These passes remove side effects from expressions
and distinguish \emph{nonaddressable} local variables from
\emph{addressable} locals.\footnote{Xavier Leroy
added the \lstinline|SimplLocals| pass to CompCert 1.12 at our request,
pulling nonaddressable locals out of memory in C light.
Prior to 1.12, source-level reasoning about local
variables (represented as memory blocks) was  much more difficult.} We recommend
that the user do this in their C code, however, so that the C light translation
will exactly match the original program.

C has pointers and permits pointer dereference in subexpressions: 
\begin{lstlisting}[language=C]
d = p->head+q->head
\end{lstlisting}

Traditional Hoare logic is not well suited for pointer-manipulating programs,
so we use a separation logic (Chapter \ref{ch:canonical}, with assertions such as 
$(p\!\!\rightarrow \! \! \mathrm{head} \mapsto x) *(q \!\! \rightarrow \! \!
\mathrm{head} \mapsto y)$. Separation logic does not permit pointer-dereference
in subexpressions, so to reason about 
\lstinline[language=C]|d = p->head+q->head| 
the programmer should factor into:
\lstinline[language=c]|t = p->head; u = q->head; d=t+u;|
where dereferences occur only at top-level in assignment commands.
%Because this is a requirement of our logic, and not of CompCert C, this
%\emph{must} be done by the user or they will get stuck during their proof.
Adding these restrictions to C light gives us \emph{Verifiable C}, which is not
a different semantics but a proper sublanguage, enforced by our typechecker.

A well typed C program might still go wrong. These are the cases where the
typechecker must generate assertions. A few of these cases might be surprising,
even to experienced C programmers.
The following operations are undefined in the C standard,
and \emph{stuck} in CompCert C:
\begin {itemize}
  \item shifting an integer value by more than the word size,
  \item dividing the minimum int by $-1$  (overflows),
  \item subtracting two pointers with different base addresses (i.e.,
from different malloc'ed blocks or from different addressable local
variables),
  \item casting a float to an int when the float is out of integer range,
  \item dereferencing a null pointer, and
  \item using an uninitialized variable.
\end{itemize}

Some operations, like overflow on integer addition, are undefined in the C standard but defined in
CompCert. The typechecker permits these cases. In summary Clight is a
subset of C where every C representation has a Clight representation
using only local modifications to the source. Our typechecker
restricts code to a subset of Clight that is friendly to
verification. 

\section{Typechecker} 
\label{sec:typechecker}
The typechecker produces assertions that, if satisfied, prove that an
expression will always evaluate to a value.  

In the C light abstract syntax produced by CompCert from C source code,
every subexpression is syntactically annotated
with a C-language type, accessed by \lstinline{(typeof e)}. 
Thus our typing judgment does not need to be of the
form $\Delta \vdash e : \tau$, it can be
$\Delta \vdash e $, meaning that $e$ typechecks according to its
own annotation.

We define a function to typecheck expressions with respect to a type context:

\begin{lstlisting}
Fixpoint typecheck_expr ($\Delta$ : tycontext) (e: expr) : tc_assert :=
  let tcr := typecheck_expr $\Delta$ in $$ match e with
    | Econst_int _ (Tint _ _ _) => tc_TT 
    | Eunop op a ty => tc_andp 
         (tc_bool (isUnOpResultType op a ty) (op_result_type e)) (tcr a) 
    | Ebinop op a1 a2 ty => tc_andp 
         (tc_andp (isBinOpResultType op a1 a2 ty)  (tcr a1)) (tcr a2)
 ... end.
\end{lstlisting}

\noindent This function traverses expressions emitting conditions that
ensure that the expressions will evaluate to a value in a correctly
typed environment. The typechecker is actually a mutually recursive
function: one function typechecks rvalues and the other typechecks
lvalues.  For convenience, this paper only discuss rvalues.  Although
CompCert's operational semantics are written as an inductive Coq type,
they also have parts that are computational. For example, when we need
to typecheck expressions that perform mathematical operations, we use functions from CompCert
that \emph{classify} them. The following function leverages
the classification function to typecheck binary operations: 
\begin{lstlisting}
Definition isBinOpResultType op a1 a2 ty : tc_assert :=
match op with
  | Oadd => match classify_add (typeof a1) (typeof a2) with 
            | add_default => tc_FF
            | add_case_ii _ $\hspace{.5mm}$ => tc_bool (is_int_type ty) 
            | add_case_pi _ $\hspace{.5mm}$ _ => tc_andp (tc_isptr a1) (tc_bool (is_pointer_type ty)) 
                   ...  end   
   ... end.
\end{lstlisting}

Most operators in C are overloaded. For example the addition operator
works on both integers and pointers, behaving differently if given two
integers, two pointers, or one of each. 
Classification functions determine which of these overloaded semantics
of operators should be used. These semantics are always determined by the types
of the operands. 
The C light operational semantics uses
the constructors (\lstinline|add_case_ii|, \lstinline|add_case_pi|, (and so on))
to choose whether to apply integer-integer add (\lstinline|ii|),
pointer-integer add (\lstinline|pi|), and so on.
The typechecker uses the same constructors
\lstinline{add_case_ii, add_case_pi,} to choose
type-checking guards, as shown above.

Despite the reuse of CompCert code on operations, the bulk of the
typechecker checks binary operations. This is because of the operator
overloading on almost every operator in C. The typechecker looks at
eight types of operations (shifts, boolean operators, and comparisons
can be grouped together as they have the exact same semantics with
respect to the type returned). Each of these has approximately four
behaviors in the semantics giving a total of around thirty cases that
need to be handled individually for binary operations.
 
The code above is a good representation of how the typechecker is
implemented. The first step is to match on the syntax. Next, if the expression
is an operation, we use CompCert's classify function to decide which overloaded
behavior to use. From there, we generate the appropriate assertion. 

\section{Type Context}
\label{sec:context}

Expression evaluation requires an expression and an environment.  The
result of expression evaluation depends on the environment. To
guarantee that certain expressions will evaluate, we control what
values can appear in environments. A type context to describes
environments where we can prove that expressions will evaluate to
defined values.

\begin{lstlisting}
Definition tycontext: Type :=
   (PTree.t (type * bool) * (PTree.t type) * type * (PTree.t global_spec)).
\end{lstlisting}
\lstinline|PTree.t($\tau$)| is CompCert's efficient computational 
mapping data-structure
(from identifiers to $\tau$) implemented
and proved correct in Coq.  
The elements of the type context are
\begin{itemize}
  \item a mapping from temp-var names to type and initialization
  information,
  \item a mapping from local variable names to types,
  \item a return type for the current function, and
  \item a mapping from global variable names to types
  \item a mapping from global variable names to function specifications
\end{itemize}

The first, second, and fourth items match exactly with the three parts of
an environment (\lstinline|environ|), which is made up of temporary variable
mappings, local variable mappings, and global variable mappings.

The fifth item is specific to proofs and is separated from the fourth
because function specifications can't be computed on. The reasoning
for this will be discussed in Chapter \ref{ch:reflection}. 

A temporary variable is a (local) variable whose address is not taken
anywhere in the procedure.  Unlike local and global variables,
these variables can not alias---so we can statically determine when their
values are modified. If the typechecker sees that a temporary variable
is initialized, it knows that it will stay initialized. If the
typechecker is unsure, it can emit an assertion guard for the user to
prove initialization. Calculating initialization automatically is a
significant convenience for the user; proofs in the previous
generation of our program logic were littered with definedness
assertions in invariants.

The initialization information is a Boolean that tells us if a
temporary variable has certainly been initialized. The rules for this
are simple, if a variable is assigned to, that variable will always be
initialized in code executed after it.  The initialization status on
leaving an \lstinline|if-then-else| is the GLB of the two
branches. Loops have similar rules.


\lstinline|typecheck_environ| checks an \lstinline|environ| with respect to
a \lstinline|tycontext|. 
It does not generate assertions as
\lstinline|typecheck_expr| does, it simply returns a Boolean
that if \lstinline{true} guarantees all of the following:
\begin{itemize}
  \item If the type context contains type information for a temporary variable
  the environment contains a value for that variable. If
  the variable is claimed to be initialized, that value
  must belong to the type claimed in the type environment.
  \item If the type context contains type information for a (addressable) local
  variable, the local variable environment contains a local variable of matching type.
  \item If the type context contains type information for a global variable, the
  global environment contains a global variable of matching type.
  \item If the type context contains type information for a global variable,
  either
     \begin{itemize}
       \item the local variable environment does not have a value for that
       variable or
       \item the type context has type information for that variable as a local
       variable.
       \end{itemize}
\end{itemize}

\noindent The fourth point is required because local variables shadow global
variables.
%Consider the program

%\begin{lstlisting}[language = c]
%float *x; $$ $$ void* bad_function( ) {int x;    return (void *) x;}
%\end{lstlisting}

%If we allow the environment to only contain information about the global
%(\lstinline[language=c]|float *|) variable \lstinline|x| and not the
%local (\lstinline[language=c]|int|) variable \lstinline|x|, this
%program will typecheck. It will typecheck because the type context is allowed
%% to be smaller than the dynamic environment and we believe the
%\lstinline[language=c]|return| expression is casting between pointers.
%We need to disallow casts from int to pointer in our
%typesystem, however, so we don't want this function to typecheck. The fourth
%point above prevents this situation by requiring a variable that is shadowed in
%a dynamic environment to also be shadowed in the type context.

Initialization information is changed by statements. We only know a variable is
initialized once we see that it is assigned to. The typechecker operates at
the level of expressions, so the logic rules which work at the
statement level must maintain the type context. We will now give some of these
rules and explain how they work to keep the type context correct.

We provide a function

\begin{lstlisting}
Definition func_tycontext (func: function) (V: varspecs) (G: funspecs): tycontext 
\end{lstlisting}
 
\noindent that automatically builds a correct type context 
(for the beginning of the function body) given the function,
local, and global specifications. The resulting context contains every variable
used in the function matched with its correct type. We have proved that the
environment created by the operational semantics when entering a function body
typechecks with respect to the context generated by this function. 
Once the environment is created, the Hoare rules use the function
\lstinline|updatetycon| to maintain the type context across statements. 
\begin{mathpar}
\inferrule{
\tripleD{\Delta}{P}{c}{Q} \\ \Delta' = \mathsf{updatetycon(\Delta,c)} \\
\tripleD{\Delta'}{Q}{d}{R}} {
\tripleD{\Delta}{P}{c;d}{R}}\mbox{seq}
\end{mathpar}
 
\lstinline|updatetycon| tells us that variables are known to be initialized
after they are assigned to. It also says that variables are initialized if they
were initialized before the execution of any statement, and that a
variable is initialized if we knew it was initialized at the end of both
branches of a preceding \lstinline|if| statement. When we say initialized, we
mean \emph{unambiguously} initialized, meaning that it will be initialized
during all possible executions of the program.

The type context is deeply integrated with the logic rules. We write our hoare
judgment as
$\triple{\Delta}{P}{c}{Q}$. It contains the type context $\Delta$ because instead
of quantifying over all environments as a normal Hoare triple does, we quantify
only over environments that are well typed with respect to $\Delta$. This has a
benefit to the users of the rules: they do not need to worry about the
contents of $\Delta$, and they do not need to show that the environment
typechecks or mention  $\Delta$ explicitly in preconditions.
Our \emph{rule of consequence} illustrates
what we always know about $\Delta$:
\begin{mathpar}
\inferrule{
\mathsf{typecheck\_environ}(\rho,\Delta)\wedge P\vdash P'\qquad
\triple{\Delta}{P'}{c}{R}}
{\triple{\Delta}{P}{c}{R}}
\end{mathpar}

The conjunct $\mathsf{typecheck\_environ}(\rho,\Delta)$
gives the user more information to work
with in proving the goal. Without this, the user 
would need to explicitly strengthen assertions and loop invariants
to keep track of the initialization status of variables
and the types of values contained therein.

With \lstinline|func_tycontext| and \lstinline|updatetycon| the rules
can guarantee that the type context is sound throughout every
proof. To keep the type context updated, the user must simply apply
the normal Hoare rules, with our special Hoare rule for statement
sequencing shown above.

 
\section{Soundness}
\label{sec:tc_sound}
The soundness statement for the typechecker is:

If the dynamic environment $\rho$ is well typed with respect to the
static type context $\Delta$ (Section \ref{sec:context}), and the expression
$e$ typechecks with respect to $\Delta$ producing an assertion that in turn is satisfied in $\rho$, 
then the value we get
from evaluating $e$ in $\rho$ (Section \ref{sec:evaluation})
will match the type that $e$ is labeled with. 

\begin{lstlisting}
typecheck_environ $\rho$ $\Delta$ = true -> denote_tc_assert (typecheck_expr $\Delta$ e) $\rho$ ->
typecheck_val (eval_expr e $\rho$) (typeof e) = true.
\end{lstlisting} 

The \lstinline|typecheck_val| function guarantees that an expression
will evaluate to the right kind of value: integer, or float, or
pointer. It also gives a range on the value that can be expected from
an integer as opposed to a short or a boolean.  As a corollary we
guarantee the absence of \lstinline|Vundef|, which has no type.

The proof proceeds by induction on the expression.  One of the most
difficult parts of the soundness proof is the proofs about binary
operations. We need to prove that when a binary operation typechecks
it evaluates to a value as a case for the main soundness proof. The
proof is difficult because of the number of cases. When all integers
and floats of different sizes and signedness are taken into account,
there are seventeen different CompCert types. This means that there
are 289 combinations of two types. A proof needs to be completed for
each combination of types for all seventeen C light operators, leading
to a total of 4913 cases that need to be proved.  Each proof requires
a decent amount of work, so the amount of memory taken by the proof
becomes a problem. We use lemmas to group some of the cases together
to keep the proof time reasonable.

These cases are not all written by hand: we automate using Ltac.
Still, the proofs are large, and Coq takes almost 4 minutes to process
the file containing the binary operation proofs.


\section{A Tactical Proof}
\label{sec:example}
This section gives an example of applying the C light program logic to
verify a simple C program interactively in Coq, verifying the C
program:

\begin{lstlisting}[language=c]
int assigns (int a) { int b, c; c = a*a; b = c/3; return b; }
\end{lstlisting}

The first step is to pass the program through the CompCert
\lstinline|clightgen| tool to create a Coq .v file. The next step is
to specify the program:

\[\tripleD{\Delta}{\mathsf{Vint}(v)=\mathrm{eval}\,\mathsf{a}\,\rho}
{\mathsf{assigns}\ldots}{\mathsf{retval}\,\rho = \mathsf{Vint}((v * v) /
3)}\]

Barring any unexpected evaluation errors, this specification should hold.
The specification states that in an arbitrary initial state, the program will either
infinite loop or terminate in a state in which \lstinline|retval = (a*a)/3|. This
example focuses on proving the specification of the function
body. The following shows
how automated the entire proof is, it will be broken down step by step
next:

\begin{lstlisting}
Lemma body_test : semax_body Vprog Gtot f_assigns assign_spec.
Proof. start_function. name a _a. name b _b. name c _c.
       forward. forward. entailer!. forward. entailer. 
       (* ... prove that the function-body postcondition implies the
       function-specification postcondition ... *) Qed.
\end{lstlisting}

In fact, the entire proof could easily be completed by one tactic:

\begin{lstlisting}
repeat (forward; try solve[entailer!])
\end{lstlisting}

For understanding the steps are listed here.

The function \lstinline|semax_body| creates a triple for a function body given a
list of global variable specifications (\lstinline|Vprog|, the empty list), a
list of global function specifications (\lstinline|Gtot|, list of this function and
main), the pointer to the function (\lstinline|f_assigns|, pointer from
program .v file), and a specification (\lstinline|assign_spec|, the Coq version
of the triple shown above). The tactic \lstinline|start_function| unfolds
\lstinline|semax_body| and ensures that the precondition is in a usable form.
The relation \lstinline|semax| defines the Hoare triple.

The \lstinline|name| tactic, and the \lstinline|name| hypotheses it
generates, relate variable names to value names. For example
\lstinline|_a| is the name of the variable \lstinline|a| in the
program. The tactic \lstinline|name a _a| tells the tactics that
values associated with evaluating \lstinline|_a| should be called
\lstinline|a|. This is strictly a convenience so that automatically
created value names won't be confusing during the proof. Without a
hint, Coq would default to calling them $x_0, x_1$ and so on. 


In the following examples, C light AST is replaced with C-like syntax
in the lines marked \lstinline|(* pseudocode *)|. Assertions are in a
canonical form, separated into \lstinline|PROP| (propositions that
don't require state), \lstinline|LOCAL| (assertions lifted over the
local environment), and \lstinline|SEP| (separation assertions over
memory) a discussion of this form, and a method for improving it can
be found in Chapter \ref{ch:canonical}. Empty assertions for any of
these mean \lstinline|True|.

\begin{lstlisting}
  a : name _a
  b : name _b
  c : name _c
  $\Delta$ := initialized _c (func_tycontext f_assigns Vprog Gtot) : tycontext
  ============================
   semax $\Delta$  (PROP  () $$ 
   LOCAL (`eq (eval_id _c) (eval_expr(_a * _a)); (`eq (eval_id _a) v)) $$ SEP ()) 
      (_b = _c / 3; return _b;)   (* pseudocode standing for C-light AST *) 
      (function_body_ret_assert tint (_a * _a / 3)  = retval)
\end{lstlisting}

Above is the state after applying \lstinline|forward| for the first
time.  This tactic performs forward symbolic execution using Coq
tactic programs, as various authors have demonstrated
\cite{appel06,chlipala11:pldi,bengtson12,mccreight09}. In effect,
\lstinline|forward| applies the appropriate Hoare rule for the next
command, using the sequence rule if necessary. The backtick
(\lstinline|`|) is the ``lifting'' coercion of Bengtson \emph{et al.}
\cite{bengtson12}.  \lstinline|function_body_ret_assert| tells us that
our postcondition talks about the state when the program returns
successfully. A program that does not return successfully will not
satisfy this triple.

The \lstinline|forward| tactic makes a decision when it sees an assignment. In
general, it uses the Floyd assignment rule that existentially quantifies the
``old'' value of the variable being assigned to (in this case \lstinline|c|). It
needs to do this because otherwise we would lose information from our
precondition by losing the old value of \lstinline|c|. This means the
postcondition would end up in the form "\lstinline|exists old, ...|''. If the
variable doesn't appear in the precondition, however, the existential can be
removed because it will never be used. The \lstinline|forward| tactic checks to
see if it needs to record the old value of the variable or not. In this case,
it sees that \lstinline|c| is not in the precondition and does not record its
old value.

In the proof so far (after symbolic execution of the
command \lstinline{c=a*a;})
there has not been a typechecking side condition---not 
because they were automatically
solved, but because they were never generated in the first place. They were checked
computationally, but no assertion about them is given. The condition that $a$ be
initialized immediately evaluates to \lstinline|True| and is dispelled trivially.  

Finally we notice that $\Delta$ has been updated with 
\lstinline|initialized _c|. This was done by the sequence rule as discussed in
Section \ref{sec:context}.

Applying \lstinline|forward| again gives 
the following separation-logic side condition: 

\begin{lstlisting}
  a : name _a
  b : name _b
  c : name _c
  $\Delta$ := initialized _b (initialized _c (func_tycontext f_assigns Vprog Gtot) : tycontext 
  ============================
   PROP() $$ LOCAL(tc_environ $\Delta$; `eq (eval_id _c) (eval_expr (_a * _a)); (`eq (eval_id _a) v)) 
   SEP(`TT) |-- local (tc_expr $\Delta$ (_c /  3)) && local (tc_temp_id _b tint $\Delta$)
\end{lstlisting}
\noindent This is an entailment, asking to prove the right hand side given
the left hand side. We need to show that the expression on the right hand side
of the assignment typechecks, and that the id on the left side typechecks. We
would expect to see that: $c$ is initialized, $3\neq 0$ and
$\texttildelow(c=\mathsf{min\_int} \wedge 3 = -1)$.

\emph{Why is it useful to have} \lstinline{tc_environ $\Delta$}?
This entailment is \emph{lifted} (and quantified) over an abstract
\lstinline|environ| $\rho$; if we were to \lstinline|intro $\rho$| and make it
explicit, then we would have conditions about \lstinline{eval_id _c $\rho$},
and so on.  
To prove these entailments, we need to know that
\lstinline{(eval_id _a $\rho$)}  and 
\lstinline{(eval_id _c $\rho$)}  are defined and well-typed.

In a paper proof it is convenient to think of an 
integer \emph{variable} \lstinline{_a}
as if it were the same as the \emph{value} obtained when looking 
\lstinline{_a} up in
environment $\rho$---we write this (\lstinline|eval_id _a $\rho$|). In general,
we can not think this way about C programs because in an arbitrary environment,
\lstinline|_a| may be of the incorrect type or uninitialized. In an environment
$\rho$ that typechecks with respect to some context $\Delta$, however, we can bring this
way of thinking back to the user. 
Our automatic \lstinline|go_lower| tactic, after introducing $\rho$,
uses the \lstinline{name} hints to replace every use
of (\lstinline|eval_id _a $\rho$|) with simply \lstinline{a},
\emph{and} it proves a hypothesis that the value
\lstinline|a| has the expected type.  In the case of an \lstinline{int},
it does one step more: knowing that  the value 
(\lstinline|eval_id _a $\rho$|) typechecks implies it must 
be \lstinline|Vint $x$| for some $x$, so it introduces \lstinline|a|
as that value $x$.  (Again, the name \lstinline|a| is chosen from
the hint, \lstinline|a: name _a|.)
Thus, the user can think about values, not about evaluation, just
as in a paper proof.
Our \lstinline|go_lower| tactic, followed by \lstinline|normalize| for
simplification converts the entailment into

\begin{lstlisting}
  c : int
  a : int
  H0 :  Vint c = Vint (Int.mul a a) (*simplified*)
  ============================
   denote_tc_nodivover (Vint c) (Vint (Int.repr 3))
\end{lstlisting}
All we are
left with is the case that the division doesn't overflow. 
The other conditions ($c$ is initialized, $3\neq 0$ ) have 
computed to \lstinline|True| and simplified away. 
We can no longer see the variables \lstinline|_c| and \lstinline|_a|.

% Instead we see (32-bit) integer Coq variables \lstinline{c, a}.
% This is because of the work done by \lstinline|go_lower|. Proving a ``lifted''
% entailment (where the antecedent and consequent both have the type
% \lstinline{environ->predicate})
% can be done by introducing an environment variable $\rho$,
% then unlifting \cite{bengtson12}, 
% so that (for example) the precondition
% will contain \linebreak
% \lstinline|(eval_id _c $\rho$ = eval_id _a $\rho$ * eval_id _a $\rho$)|.
% Our tactics have then replaced all occurrences of
% \lstinline|(eval_id _c $\rho$)| with simply \lstinline{c},
% using the \lstinline|name| hypotheses to choose
% the variable name \lstinline{c}.
% The fact that \lstinline|c| must be a defined (initialized)
% \lstinline|int| value is proved by \lstinline|go_lower|
% using information from the typechecker.

Now we can apply some simple Boolean
rewrite rules with \lstinline|solve_tc| and solve the goal.
Not all typechecker-introduced assertions will be so easy
to solve, of course; in place of \lstinline|solve_tc|
the user might have to do some real work.

The rest of the proof advances through the return statement, then proves that
the postcondition after the return matches the postcondition for the 
specification. In this case it is easy, just a few unfolds and rewrites.


\section{Related Work}
Frama-C is a framework for verifying C programs
\cite{cuoq2012frama}. It presents a unified assertion language to
enable static analysis cooperation. The assertion language allows
users to specify only first-order properties about programs, and does
not include separation logic. The Value analysis \cite{canet2009value}
uses abstract interpretation to determine possible values, giving
errors for programs that might have runtime errors. The WP plugin uses
weakest precondition calculus to verify triples.  WP is only correct
when first running Value which may result in some verification
conditions that can then be verified by WP along with the function
specifications. Frama-C does not seem to have any soundness proof.
Frama-C is an incredibly useful tool for finding bugs and specifying
programs without putting forth too much effort. The level of assurance
and the strength of logic that it provides, however, are of a
different level than VST. It is likely that in proving a program
correct it would be an efficient to first use Frama-C to get the
program as close to correct as possible and then use VST to give
complete assurance of correctness.

VCC is a verifier for concurrent C programs. It works by translating C programs to 
Boogie, which is a combination of an intermediate language, a VC generator, and an
interface to pass VCs off to SMT solvers such as Z3.
VCC adds verification conditions that ensure that expressions
evaluate. Like the Value analysis, there is no link between the
conditions and an operational semantics, they are what the developers
thought of when writing the program. There is no guarantee that the
conditions generated are sufficient to keep the program from crashing
when run.

Greenaway \emph{et al.} \cite{greenaway12} show a verified conversion from C
into a high-level specification that is easy for users to read. They do this
by representing the high-level specification in a monadic language.
They add guards during their translation out of C in order to ensure
expression evaluation (this is done by Norrish's C parser
\cite{norrish:parser}). Many of these guards will eventually be removed
automatically. Their tool is proved correct with respect to the semantics of an
intermediate language, not the semantics of C. The expression evaluation guards are there
to ensure that expressions always evaluate in the translated program, because
there is no concept of undefined operations in the intermediate language.
Without formalized C semantics, however, the insertion of guards must be
trusted to actually do this. This differs from our approach where the
typechecker is proved sound with respect to a C operational semantics;
so we have more certainty that we have found all the necessary side conditions.
Another difference is that they produce all the additional guards and then
solve most of them automatically,  while we avoid creating
most such assertions. Expression evaluation
is not the main focus of Greenaway's work, however, and the ideas presented for
simplifying C programs could be useful in conjunction with our work,
possibly to automatically generate parts of specifications that can be
used as a starting point for VST users to create program
specifications.

Bengtson \emph{et al.} \cite{bengtson12} provide a framework for verifying
correctness of Java-like programs with a higher-order separation logic similar
to the one we use. They use a number of Coq tactics to greatly simplify
interactive proofs. Chlipala's Bedrock project
\cite{chlipala11:pldi} also aims to decrease the tedium of separation logic
proofs in Coq, with a focus on tactical- and reflection-based 
automation of proofs about low level programs. Bengtson operates on a Java-like
language and Chlipala uses a simple but expressive low-level
continuation-based language.
%Both of these projects focus on the length of the proof
%but don't mention how quickly the proofs compile in Coq. 
Earlier versions of our
work (Appel \cite{appel06}) used a number of
tactics to automate proofs as well. In this system, the user was left with the
burden of completing proofs of expression evaluation.

The proof rules we use in this paper are also used in the implementation of a
verified symbolic execution called VeriSmall \cite{appel11:verismall}. 
VeriSmall does efficient, completely automatic 
shape analysis.  

Tuerk's HolFoot \cite{Tuer09} is a tactical system in HOL for
separation logic proofs in an imperative language.  Tuerk uses an
idealized language ``designed to resemble C,'' so he did not have to
address many of the issues that our typechecker resolves.

One of Tuerk's significant claims for HolFoot is that his tactics
solve purely shape-analysis proofs without any user assistance, and as
a program specification is strengthened from shape properties to
correctness properties, the system smoothly ``degrades'' leaving more
proof obligations for the interactive user.  This is a good thing.  As
we improve our typechecker to include more static analysis, we hope to
achieve the same property, with the important improvement that the
static analysis will run much faster (as it is fully reified), and
only the user's proof goals will need the tactic system.

Our implementation of computational evaluation is similar to work on
executable C semantics by Ellison and Rosu \cite{ellison-rosu-2012-popl} or
Campbell \cite{Campbell-cpp-12}. Their goals are different, however.
Campbell, for example, used his implementation to find bugs in the
specification of the CompCert semantics. We, on the other hand, are accepting
the CompCert semantics as the specification of the language we are operating
on. Ellison and Rosu have the goal of showing program correctness, which is a 
similar goal to ours. They show program correctness by using their semantics as
a debugger or an engine for symbolic execution. 


\chapter{Canonical Forms for Assertions}
\label{ch:canonical}

VST uses assertions to reason about programs, but what does it require
about the form of these assertions? This chapter discusses that, as
well as how restricting the form of assertions improve both the
usability and the performance of symbolic execution.

All assertions in VST's logic take an \lstinline|environ| as an
argument. This is because the environment will remain fully abstract
during the symbolic execution performed using the assertions. The
symbolic execution proceeds not by inserting symbolic values into an
environment, but by writing (likely symbolic) assertions about the
values. To do this, we simply add an assertion about the value of a
variable when it is looked up in the environment. The function for
looking up an unaddressed variable in an envionment is
\lstinline|eval_id|. We can write an assertion of type
\lstinline|environ -> Prop| about variable $x$ in environment $rho$:

\begin{lstlisting}
(fun $\rho$ => eval_id $x$ $\rho$ = 4)
\end{lstlisting}

Or equivalently, hiding the environment by using the lifting operator:

\begin{lstlisting}
`eq (eval_id $x$) `4
\end{lstlisting}

Simply referring to the environment is not enough to specify a C
program because C programs also refer to the heap. An assertion in
VST's logic can be any semantic predicate of type 
\lstinline|environ -> mpred|. 
\lstinline|mpred| is the type of separation logic assertions over C
memory. Separation logic has operators that make it particularly
convenient to reason about programs that might have pointer
aliasing. It does this by providing the \emph{separating conjunction}
( * ) operator, which means that if there are two pointers $p_1$ and
$p_2$ that point at values $v_1$ and $v_2$ (using the \emph{mapsto}
operator $\mapsto$) the assertion $p_1 \mapsto v_1 * p_2 \mapsto v_2$
means that $v_1$ and $v_2$ must be located at different positions in
the heap. We can also say that it tells us that $p_1$ and $p_2$ are
not aliases for each other. More formally, a heap that satisfies
$P * Q$ can be partitioned into two sub-heaps, one satisfying $P$ and
the other satisfying $Q$.

This is very useful in C programs because the following triple holds

$\triple{p_1 \mapsto v_1 * p_2 \mapsto v_2}{p_1 := v_3}{p_1 \mapsto v_3 * p_2 \mapsto v_2}$

We only had to update one part of the assertion because we were
certain that $p_1$ and $p_2$ point to different locations in memory. If
we had used standard conjunction we would have needed to know the fact
$p_1 \neq p_2$. That fact isn't a problem in a small examples, but the
number of such inequalities you would need to know is quadratic with
the number of references to variables in the assertion. 

Of course, not all parts of an assertion need to be about memory. Some
parts might just refer to the environment, in which case we use the
\lstinline|local : (environ -> Prop) -> (environ -> mpred)|
operator. We might also need pure facts which we denote with
\lstinline|!! : Prop -> (environ -> mpred)|. To combine pure facts
with separation facts we can use the \lstinline|&&| operator which is simply the
and operator on \lstinline|mpred|. Putting those together we can write
an assertion such as

\begin{lstlisting}
!!(x = 3) &&
local (`eq (eval_id _x)  `x) &&
(p1 $\mapsto$ x * p2 $\mapsto$ 5)cc
\end{lstlisting}

Appel et al. organized these assertions into a $\PROP$/$\LOCAL$/$\SEP$
form to make them easier to manipulate in proofs. I will describe this
\emph{semicanoncial form} in section \ref{sec:scform}. Then I will
improve on that form, which will makes proof easier to automate than
they were in semicanonical form. This \emph{canonical form} is
described in Section \ref{sec:cform}

\section{Semicanonical Form}
\label{sec:scform}
We say that assertions written~\lstinline{PROP $~P$ LOCAL $~Q$ SEP $~R$}  
are in semicanonical form. Here each item in
\lstinline|$P$:list prop| is a pure assertion that doesn't refer to
the program state. \lstinline|$Q$: list (environ -> prop)| contains
assertions that can reason about local variables.  and 
\lstinline|$R$: list (environ -> mpred)| has spatial assertions that can reason about
both local variables and memory.  $\PROP$ folds conjunction over the
elements of $P$, $\LOCAL$ folds lifted conjunction ( 
\lstinline|`and : (environ -> Prop)-> (environ -> Prop) -> (environ -> Prop))|, and
$\SEP$ folds the separation logic \lstinline|*|, or separating
conjunction operator. $P$, $Q$, and $R$, are represented as lists
because, particularly in the case of $\SEP$, it is convenient to refer
to the $n$th conjunct. This is much easier to implement when our
assertion is restricted to a flat sequence of conjunctions. The
precise definition of \lstinline{PROP $~P$ LOCAL $~Q$ SEP $~R$} is:
\pagebreak
\begin{lstlisting}
!!(fold_right ( $\wedge$ ) True P) &&
local(fold_right ( `$\wedge$ ) `True Q) &&
(fold_right ( * ) emp R)
\end{lstlisting}

The lowest level Verifiable C logic rules are agnostic to any
structure that the assertion might have. The rules refer to
assertions as single variables, using entailments to constrain them
rather than imposing syntactic requirements on them. The Floyd
automation system has higher level lemmas that require assertions
to be in semicanonical form, but also guarantee that the side
conditions that result from using the rules will be in semicanonical
form.

\subsection{Substitution in semicanonical form}

At the Verifiable C level, there is no choice but to use a semantic
notion of substitution called \linebreak
\lstinline|subst {A: Type} (x : ident) (v:val) (P : environ -> A) : environ -> A|.
This is because we know nothing about the assertion at all, only that
it takes an environment and returns an \lstinline|mpred|. The following example
shows how \lstinline|subst| is used:

\begin{lstlisting}
$\inference[semax\_set\_forward]{}{
\Delta\vdash\triple{\later P}{~x:=e~}{\exists v.\,x=(e[v/x])\wedge P[v/x]}
}$

Axiom semax_set_forward: $~~$forall $\Delta$ ($P$: environ->mpred) ($x$: ident) ($e$: expr),
  semax $\Delta$
    (|> (local (tc_expr $\Delta$ $e$) && local (tc_temp_id id (typeof $e$) $\Delta$ $e$) && $P$))
    (Sset $x$ $e$) 
    (normal_ret_assert 
      (EX old:val, local (`eq (eval_id $x$) (subst $x$ (`old) (eval_expr $e$)))
                    && subst $x$ (`old) $P$)).
\end{lstlisting}

There are two substitutions here, used to replace any occurrences of
the variable \lstinline|x| that might have occurred in either the
precondition or the expression being assigned into \lstinline|x|. To
see why the substitution is necessary, the following example shows
what happens if we don't have substitution:

\begin{minted}{coq}
{eval_id x = 3}
   x = 4;
{eval_id x = 3 /\ eval_id x = 4}
\end{minted}

For the triple above to hold, the program would need to infinite loop
on the assignment to \lstinline|x|, which seems unlikely. Instead we
do the substitution and get:

\begin{minted}{coq}
{eval_id x = 3}
   x = 4;
{exists x_old, x_old = 3 /\ eval_id x = 4}
\end{minted}


Although subst is a function, in practice it can never be computed.
This is because it works by updating the environment that $P$ refers
to. During symbolic execution, however, the environment is always
abstract, constrained only by the precondition, which means there is
no datastructure to perform updates in. This means that the definition of
\lstinline|subst| that appears in Verifiable C isn't directly useful
to proof automation. It can't compute so without special lemmas and
tactics it will appear in side conditions. To deal with this the Floyd
system has an autorewrite database that lets it push subst through
functions that won't be affected by the substitution. For example

\begin{minted}{coq}
Lemma subst_sepcon: forall i v (P Q: environ->mpred),
  subst i v (P * Q) = (subst i v P * subst i v Q).
\end{minted}

Fortunately, we don't need a lemma for every function that might
appear in assertions. Lifted functions can't do anything with the
environment, they can only pass it on to their arguments, so
by creating autorewrite rules for lifted functions we cover
most of the functions that we use, and also most functions
that a user might want to write. 

Semantic substitution is still inconvenient for a few reasons. First,
the rewrite rules aren't complete. This means that in some cases, after
applying a logic rule, the user will see a \lstinline|subst| in a
resulting condition. This can stop the automated entailment
solvers from working correctly and make the assertion much harder
to read. The next problem is an issue with autorewrite in general.
Autorewrite in Coq is slow. Rewrites aren't known for their 
performance, and autorewrite can do a large number of rewrites
(in the case of \lstinline|subst| the number of rewrites is
linear in the size of the assertion being rewritten). 

There is a situation when a substitution \lstinline|subst $x$ $v$ $P$| can
be avoided completely. That is when $P$ is \emph{closed} wrt. 
$x$, also a semantic notion:

\begin{minted}{coq}
Definition closed_wrt_vars {B} (S: ident -> Prop) (F: environ -> B) : Prop := 
  forall rho te',  
     (forall i, S i \/ Map.get (te_of rho) i = Map.get te' i) ->
     F rho = F (mkEnviron (ge_of rho) (ve_of rho) te').
\end{minted}

Generally we give \lstinline|S| as Coq equality with a specific
identifier.  What \lstinline|closed_wrt_vars| means, then, is that if
\lstinline|F| is supplied an environment that is the same at all
locations but the identifier(s) \lstinline|i| that satisfy
\lstinline|S|, the result of \lstinline|F| will be the same. That
means that if we know \lstinline|closed_wrt_vars (eq x) (e)|, we can
easily prove \lstinline|subst x _ e = e|. More intuitively, if an
expression doesn't contain a variable, a substitution on that variable
won't change the expression.

Floyd has a set lemma that takes advantage of this, stating that if
the precondition and the expression in the assignment are closed wrt
the variable being assigned into, no substitutions are needed, but
there are numerous cases where this rule doesn't apply, so the
substitution will still appear.

Semicanonical form is very convenient for the user when
\emph{writing} assertions. The list notation is great for combining
assertions without having to remember the exact conjunction that must
be used for each part. It also allows the $\LOCAL$ to remain small,
because if there is a variable the user knows nothing about, there is
no need to add it to the locals. It is less convenient when moving
through a proof of a program. It can introduce existentials and
substitutions that are slow to simplify, or sometimes don't simplify
at all.


\section{Canonical Form}
\label{sec:cform}
The reason that substitutions are difficult, and that they need to be
semantic is because there is no \emph{syntactic} restriction on where
any individual identifier can appear within an assertion.  Canonical
form imposes such a restriction, and in doing so, eliminates the need
for semantic substitution, replacing it with a more efficient and
convenient computational syntactic substitution.

One limitation of canonical form is that we no longer allow references
to C program variables in the part of the assertion that contains
spatial assertions. If these assertions wish to talk about those
variables, they must do it indirectly using a Coq variable.  This
means that only the $\LOCAL$ part of the assertion has the ability to
reference local variables. This still doesn't give us the ability to
syntactically locate each reference though, so we restrict $\LOCAL$
further. The restriction we use is to change the entirety of $\LOCAL$
into two computational maps from identifiers to values.  One of these
represents temporary or nonadressable variables, and the other
represents addressable variables. Each mapping represents an equality
between the evaluation of an identifier in the environment, and the
value it maps to. The mappings are represented by PTrees, an efficient
computational data structure in the Coq standard library.

With these two changes we get \emph{canonical form.}  Let
\lstinline{$T_1$: PTree val} be a computational map from C program
identifiers to C values, representing the current values of the
temporary local variables of the current program state. Let
\lstinline{$T_2$: PTree (type*val)} be a map from identifiers to
\lstinline{type*val} representing the addresses of addressable local
variables.  Then \lstinline{localD $T_1$ $T_2$: list(environ->Prop)}
means a list of assertions about the contents of the
\lstinline{environ}, the nonmemory portion of the program state; we do
not need \emph{arbitrary} assertions of type
\lstinline{list(environ->Prop)}.

\lstinline{localD} is a \emph{denotation function}, reflecting the
syntactic (computationally oriented) $T_1$ and $T_2$ back into our
semantic world.  In symbolic execution and efficient entailment
solving, we operate directly on $T_1$ and $T_2$, reflecting the
results back only when the less efficient (but easier to understand)
semantic view is needed by the user. Now a full assertion is:

\begin{lstlisting}
assertD $P$ (localD $T_1$ $T_2$) $R$ : environ->mpred
$P$ : list prop$\qquad$ $T_1$ : PTree val$\qquad$ $T_2$ : PTree (type * val)$\qquad$ $R$ : list mpred
\end{lstlisting}

\subsection{Substitution in Canonical Form}

Substitution in this assertion is as simple as adding/replacing a
mapping in $T_1$ or $T_2$. To see why imagine that we are doing a
substitution on a temporary variable $x$.  \lstinline|$P$ : list prop|
and \lstinline|$R$ : list mpred| don't refer to an environment, so
they are trivially closed wrt.  $x$. This leaves $T_1$ and $T_2$. The
variable $x$ is a temporary variable, so we know $T_2$ is closed
wrt. $x$.  The map $T_1$, however, might have a reference to $x$,
meaning we actually need to do a substitution, making sure to replace
every reference to that variable. One of the requirements of a Coq map
is:

\begin{minted}{coq}
Axiom PTree.gss
     : forall (A : Type) (i : positive) (x : A) (m : PTree.t A),
       PTree.get (PTree.set i x m) i = Some x
\end{minted}

This means that if we update $x$ in some PTree, the old mapping of $x$
will no longer exist, which is the exact definition we want from a
substitution. That is how you do a substitution in an assertion, but
we still need to do substitution in the arbitrary C expression that
appears in the assignment statement and turn that expression into a
value. It is simple enough to turn a C expression into an
\lstinline|environ->val| using the \lstinline|eval_expr| function
discussed in \ref{}, but that expression could have references to
identifiers, which we can't have if we want syntactic
substitution. Instead we can take advantage of the syntactic
expressions to write a standard, easily computable syntactic
substitution that implies the substitution fact we need when applied
to canonical expressions. We use this technique to create
\lstinline|msubst_eval_expr|.  When \lstinline|msubst_eval_expr| needs
to evaluate a variable it doesn't do it in an environment. Instead, it
performs the lookup in PTrees $T_1$ or $T_2$ mentioned
earlier. Instead of (first) performing a syntactic substitution and
(then) symbolic-evaluating the expression as we needed to do before
evaluating \lstinline|eval_expr|, \lstinline|msubst_eval_expr| does both in one
pass, enabled by the deeply embedded form our local assertions.  The
symbolic evaluation performed by \lstinline|msubst_eval_expr| is partial
because there might not be any information about a variable in the
assertion. So in our lemma we require \lstinline|msubst_eval_expr| to
succeed:

\begin{lstlisting}
Axiom semax_PTree_set: $~~$forall $\Delta$ id P T1 T2 R $e$ v,
  msubst_eval_expr T1 T2 $e$ = Some v ->
  semax $\Delta$
    (|> local (tc_expr $\Delta$ $e$) && local (tc_temp_id id (typeof e) $\Delta$ e) 
            && (assertD P (localD T1 T2) R))
    (Sset id $e$)
    (normal_ret_assert (assertD P (localD (PTree.set id v T1) T2) R)).
\end{lstlisting}

What that means is that for this lemma to be used, the precondition
must have mappings for every variable that appears in $e$. The
previous lemma didn't require this because it was able to use
\lstinline|eval_expr| wherever it wanted to without requiring a
complete, successful, symbolic execution. This still works for proofs
because \lstinline|`eval_expr $e$| might eventually simplify to
\lstinline|`eval_id $x$|, which could appear in other places in the
assertion.

Floyd's forward assignment rule has an existential to bind the 'old'
value of the variable.  Our \lstinline|semax_PTree_set| avoids this
existential by making sure that the only direct reference to the old
variable is replaced when the PTree is updated.  This simplifies the
proofs, as it can be inconvenient and inefficient to deal with those
existentials.

Even the current form is not completely canonical. It could be
restricted further which would improve performance in some ways, but
also inconvenience the user in others. Finding a way to sort the
$\SEP$ and keep it sorted as new conjunctions are added could lead to
very efficient and simple entailment solving. This is a harder problem
than sorting the $\LOCAL$ though, because we want $\SEP$ to contain a
variety of predicates, including predicates that are created by the
user. The other problem is that Coq variables can appear as arguments
to the predicates, making it impossible to establish an ordering over
them. Even so, any amount of canonicalization of the $\SEP$ could help
with entailment solving efficiency, so it is worth a look in the
future. The assertions might also be made more canonical by examining
the types of pure assertions that often occur and giving them a
special location. As an example, propositions about the range of
integers commonly occur, especially doing proofs about programs that
operate on arrays. Creating a special form for specific parts of pure
operations might benefit the automation by making it simple to look
for assertions about the range of a particular variable. It might also
help future decision procedures by gathering information that is
relevant to a particular type of goal. Again, this is harder to
make canonical because there is no concrete datastructure equivalent
to the identifier names in the $\LOCAL$. Instead there are only Coq
variables, which might be mixed with function applications making any
form of sorting them difficult. 



\chapter{Applying A Reflective Framework}
\label{ch:reflection}

Every technique discussed so far helps to organize proofs and improve
the performance of automation. Initially, these techniques were
implemented in the Ltac floyd automation for VST. The floyd tactics
suffer from serious performance issues, leaving frustrating delays
between many of the tactic applications. To improve the performance,
we create a reflective automation tactic, capable of advancing through
most basic blocks in a given C program.  The tactic was created with
Malecha et al.'s s \emph{Mirror Core} framework.

There are a number of operations that need to be implemented to create
a complete reflective tactic: 
\begin{enumerate}
\item a reification tactic to transform proof states into a
  syntax that can be computed on,
\item functions that operate on the reified syntax, transforming
  reified syntax into other reified syntax,
\item soundness proofs that the functions make valid transformations
  on reified proof states wrt Coq,
\item denotation functions to create proof goals from reified syntax,
  and
\item a tactic to combine these steps into a single, fast, proof state
  transformation
\end{enumerate}

We will discuss the implementation of each of these in Mirror Core,
along with the special efforts we had to make VST work within the
limitations of the reflective framework.

\section{Reification and Denotation}

The first step in using Mirror-Core is to create a tactic for
reification. Mirror-Core includes a reification plugin written in
OCaml. The plugin is modular, meaning it can be updated with rules for
translating various languages. Although the tactic can be updated with
rules for translation, there must be an existing reified syntax for
the logic. This syntax comes in two parts.

\begin{enumerate}
\item An inductive datastructure with definitions for each type and
\item an inductive datastructure with syntax for \emph{interpreted}
  functions.
\end{enumerate}

The functions are interpreted because they have meaning without
examining the denotations. Crucially, Mirror-Core can extend its
symbol table with \emph{uninterpreted} functions if it runs into a
function that it doesn't have a rule for. This allows it to work in
the general case, where all symbols aren't known at the time the
tactic is created. 

The type definition is straightforward, the only particular
requirement (guaranteed by a typeclass instance) is that an arrow type
exists:

\begin{lstlisting}
Inductive typ :=
| tyArr : typ -> typ -> typ
| tytycontext
| tylist : typ -> typ
| ...
\end{lstlisting}

There is also a denotation function that maps \lstinline|typ| to
\lstinline|Type|:

\begin{lstlisting} 
Fixpoint typD (t : typ) (*(m : PositiveMap.t Type)*): Type :=
    match t with
        | tyArr a b => typD a  -> typD b
        | tytycontext => tycontext
        | tylist t => list (typD t )
        | ...
    end.
\end{lstlisting}

This encoding allows for polymorphic types, as demonstrated by the
\lstinline|tylist| and \lstinline|tyArr| rules, which recursively take
the denotation of their type arguments.

Once the types are defined, we define and inductive syntax for
interpreted functions, which we divide into subgroups to make
organization and matching easier:

\begin{lstlisting}
Inductive const :=
| fN : nat -> const
| fZ : Z -> const
| fint : int -> const
| fint64 : int64 -> const
| ...

Inductive eval :=
| feval_cast : type -> type -> eval
| fderef_noload : type -> eval
| feval_field : type -> ident -> eval
| feval_binop : binary_operation -> type -> type -> eval
| feval_unop : unary_operation -> type -> eval
| feval_id : ident -> eval.

Inductive other :=
| feq : typ -> other
| fnone : typ -> other
| fsome : typ -> other
| ...

...

Inductive func' :=
| Const : const -> func'
| Zop : z_op -> func'
| Intop : int_op -> func'
| Value : values -> func'
| Eval_f : eval -> func'
| Other : other -> func'
| Sep : sep -> func'
| Data : data -> func'
| Smx : smx -> func'.

\end{lstlisting}

There are two types of functions to note here. The first is functions
that take \lstinline|typ| as an argument. These are polymorphic
functions that typically operate over the polymorphic types we have
already defined. The constructors \lstinline|feq|, \lstinline|fnone|
and \lstinline|fsome| above are examples of this. The other
interesting function definitions are constructors with arguments of
other types. These functions are said to take constants as
arguments. This is very convenient, because as discussed in chapter
\ref{ch:computation}, most shallowly embedded logics, including ours,
have deeply embedded sub languages that should not need to be reified
in order to be computed on. These include things like the types of C
variables and all parts of C expressions, as well as \lstinline|Z|,
\lstinline|nat|, and any other Coq datatype that might appear and be
made up completely of constructors.

Next we need a function that defines the reified type of the functions
represented by each constructor:

\begin{lstlisting}
Definition typeof_const (c : const) : typ :=
 match c with
| fN _ => tynat
| fZ _ => tyZ
| fint _ => tyint
| fint64 _ => tyint64
| ...

Definition typeof_eval (e : eval) :=
 match e with
| feval_cast _ _ => (tyArr tyval tyval)
| fderef_noload _ => (tyArr tyval tyval)
| feval_field _ _ => (tyArr tyval tyval)
| feval_binop _ _ _=> (tyArr tyval (tyArr tyval tyval))
| feval_unop _ _ => (tyArr tyval tyval)
| feval_id _  => (tyArr tyenviron tyval)
end.

Definition typeof_other (o : other) :=
match o with
| feq t => tyArr t (tyArr t typrop) 
| fnone t => tyoption t
| fsome t => tyArr t (tyoption t)
| ...
end.

Definition typeof_func' (f: func') : typ :=
match f with
| Const c => typeof_const c
| Zop z => typeof_z_op z
| Intop i => typeof_int_op i
| Value v => typeof_value v
| Eval_f e => typeof_eval e
| Other o => typeof_other o
| Sep s => typeof_sep s
| Data l => typeof_data l
| Smx t => typeof_smx t
end.
\end{lstlisting}

And then define the denotation function, which is dependently typed
based on the application of the typeof function:

\begin{lstlisting}
Definition constD (c : const)
: typD (typeof_const c) :=
match c with
| fN c | fZ c | fPos c | fident c | fCtype c | fCexpr c | fComparison c | fbool c | fint c 
| fint64 c | ffloat c | ffloat32 c | fenv c | fllrr c
                                          => c
end.

Definition otherD  (o : other) : typD  (typeof_other o) :=
match o with
| feq t => @eq (typD t) 
| fsome t => @Some (typD t)
| fnone t => @None (typD t)
| ...
end.

Definition evalD  (e : eval) : typD  (typeof_eval e) :=
match e with
| feval_id id => eval_id id
| feval_cast t1 t2 => eval_cast t1 t2
| fderef_noload t => deref_noload t
| feval_field t id => eval_field t id
| feval_binop op t1 t2 => eval_binop op t1 t2
| feval_unop op t => eval_unop op t
end.

Definition funcD  (f : func') : typD  (typeof_func f) :=
match f with
| Const c => constD  c
| Zop z => z_opD  z
| Intop i => int_opD  i
| Value v => valueD  v
| Eval_f e => evalD  e
| Other o => otherD  o
| Sep s => sepD  s
| Data l => dataD l
| Smx t => smxD t
end.
\end{lstlisting}

This is not the extent of functions that we want to be injected into
Mirror Core, however. Our function definition is actually:

\begin{lstlisting}
Definition func := (SymEnv.func + @ilfunc typ + @bilfunc typ + func')%type.
\end{lstlisting}

The first item is a symbolic environment where we can put
uninterpreted functions. The second and third are modular logics
% TODO: citation?
that might eventually allow us to take advantage of generic decision
procedures. Finally \lstinline|func'| is our syntax definition.

With this syntax created, the next step is to tell Mirror Core's
tactic how to use it. This is done by creating tables of rules which
map parts of proof goals to reified syntax. Mirror Core's reification
tactic is a Coq plugin which provides syntax for defining these rules:

\begin{lstlisting}
Reify Pattern patterns_vst_typ += (!!bool) => tybool.

Reify Pattern patterns_vst_hastype += 
      (RHasType bool (?0)) => (fun (a : id bool) 
                                       => (@Inj typ func (inr (Const (fbool a))))).

Reify Pattern patterns_vst += 
      (!!@eq @ ?0) => (fun (a : function reify_vst_typ) 
                       => @Inj typ func (inr (Other (feq a)))).
\end{lstlisting}

This selection of rules shows most of the features that can be
used. The identifiers \lstinline|patterns_vst_typ| \lstinline|patterns_vst_hastype| and
\lstinline|patterns_vst| tell the plugin which table the rule belongs
to. Elsewhere in the file, the tables are defined and given an order
that they will be tried by the reifier. The first pattern defines the
reification of the bool type. It simply matches the type
\lstinline|!!bool| where \lstinline|!!| is notation that tells the
plugin this will be an exact match by equality. When the type is
matched, it tells the reification tactic that it should emit
\lstinline|tybool|.

The second rule doesn't make an exact match, instead matching the type
of the expression to bool. The \lstinline|RHasType| pattern takes an
argument, denoted by \lstinline|?0|. The 0 means that the variable is
given to the first argument of the function on the right hand side of
the rule. The type \lstinline|id bool| of the argument means that it
will be passed a Coq bool as opposed to a reified bool. The
\lstinline|Inj| constructor allows an injection of a reified function
into Mirror Core's lambda calculus. 

The next rule demonstrates reifying a polymorphic equation. Of note is
the type of the argument \lstinline|a|. It is 
\lstinline|function reify_vst_typ| which means that it will take a Coq type and reify it
into a Mirror Core type.

The features shown here are sufficient to define reification for all
of the symbols that might be found in VST. Mirror Core comes with
built in ways to represent proof states and lemmas, as well as tactics
to reify into those representations. There is no further customization
needed, those tactics are automatically created given the
customization we have presented here. 

\subsection{Representing Dependent Types}

There is one part that is tricky though, and that is the dependently
typed definition of our separation logic predicates for dealing with
data structures:

\begin{lstlisting}
reptype (t: type) : Type
data_at : Share.t -> forall t : type, reptype t -> val -> mpred

...
| fdata_at : type -> sep (*reified data_at)*
...
\end{lstlisting}

It is dependently typed because it is a single predicate that works on
expressions that could contain any C type. Because \lstinline|data_at|
specifies the behavior of a program, \lstinline|data_at| relates C
values to Coq values. This requires the use of the \lstinline|reptype|
function, which is simply a mapping from C types to Coq
types. Unfortunately, Mirror Core doesn't support dependently typed
functions. We are able to avoid this restriction, however, because we
know that \lstinline|reptype| will always be called on a type that
comes from a \emph{real} program. That means that the function call
will always terminate and result in a constant. The \lstinline|data_at|
function with \lstinline|reptype| evaluated on an argument is no
longer a dependently typed function, so it can be reified. The tricky
part is that we can't use \lstinline|reptype| as the type of the
reified function because the reified function needs to have reified
types. So we define a function \lstinline|reptyp| (no e) with the same
implementation as \lstinline|reptype| except for every Coq type
returned in \lstinline|reptype| is exchanged for a reified
type. Here are examples from both functions:

\begin{lstlisting}
Fixpoint reptyp (ty: type) : typ :=
  match ty with
  | Tvoid => tyunit
  | Tarray t1 sz a => tylist (reptyp t1)
  ... end.

Fixpoint reptype (ty: type) : Type :=
  match ty with
  | Tvoid => unit
  | Tarray t1 sz a => list (reptype t1)
  ... end.
\end{lstlisting}

Then we say the reified type of the \lstinline|data_at| function is:

\begin{lstlisting}
Definition typeof_sep (s : sep) : typ :=
match s with
  | fdata_at t => tyArr tyshare (tyArr (reptyp t) (tyArr tyval tympred))
... end
\end{lstlisting}

But then to define the denotation function we need a proof:

\begin{lstlisting}
Definition typD_reptyp_reptype: forall t, typD  (reptyp t) = reptype t.
\end{lstlisting}

In order to convince the denotation function that the denotation
\lstinline|fdata_at| applied to the reified type obtained by
\lstinline|reptyp| will match the type obtained by \lstinline|reptype|
in the implementation of \lstinline|data_at|. This is a significant
amount of work to be put into reifying a dependently typed
function. It doesn't work in the general case either; This technique
only works if you are sure that the dependent type will compute away
when the function is applied to some number of its first
arguments. For the foreseeable future of VST, this dependent typing
capability is sufficient, but it might make Mirror Core impossible to
use in some other projects with more difficult dependent types.

\section{Reified Tactics}

A Mirror Core tactic, called an \emph{Rtac} is a Gallina program that transforms
a reified proof state into a new reified proof state. To be used as a
tactic, these programs must be proved sound with respect to the Coq
implication. That is (greatly simplified):

\begin{lstlisting}
Definition rtac_sound tac := 
forall reified_state,
goalD reified_state -> goalD (tac reified_state)
\end{lstlisting}

Where \lstinline|goalD| is a denotation function for reified proof
goals. This is simplified because it doesn't deal with the variables
and unification variables that might be created by the execution of
the tactic. This leads to a significant increase in the complexity of
the statement, but the meaning is still the same.

Mirror Core provides a number of tactics that are sound, assuming the
arguments they are applied to are also sound. 
The list below details a core set of Rtac connectives. We use 
{\it tac} to represent Mirror Core tactics and {\it lem} to represent reified lemmas.

\vspace{5mm}

\noindent \begin{tabular}{l@{\quad\quad}p{90mm}}
{\sf INTRO} & Pulls a universally quantified variable or the left hand
side of an
implication into the context (like {\sf intro}). If the head of the goal is an existential
quantifier, a unification variable is created in its place (like {\sf
  eexists}). \\
{\sf THEN} {\it tac1} {\it tac2} & Run {\it tac1} followed by running {\it
  tac2} on any resulting subgoals (like the ``{\sf ;}'' tactic). The tactic fails if either 
  {\it tac1} or {\it tac2} fails.\\
{\sf REPEAT} {\it n} {\it tac} & Runs the tactic {\it tac} {\it n}
times, or until it fails. \\
\end{tabular}

\noindent \begin{tabular}{l@{\quad\quad}p{90mm}}
{\sf TRY} {\it tac} & Runs the tactic {\sf tac} but succeeds and does
nothing when {\sf tac} fails.\\
{\sf APPLY} {\it lem} & Applies {\it lem} to the current goal. One
subgoal is created for each hypothesis of {\it lem}. All variables of
{\it lem} must be unified.\\
{\sf EAPPLY} {\it lem} & Similar to {\sf APPLY} but generates
unification variables for any unmatched variable of {\it lem} instead
of failing. \\
{\sf FAIL} & Ignore the proof state and fail. \\ 
\end{tabular}

The combination of these tactics is sufficient to create the main
structure of a tactic to progress through a C basic block. We start by
creating a tactic that will apply to a triple with a single statement:

\begin{lstlisting}
Definition APPLY_SKIP :=  (APPLY typ func  skip_lemma).
\end{lstlisting}

Where \lstinline|skip_lemma| is the automatic reification of the same
skip lemma used elsewhere in VST. Other lemmas are more
complicated. For example the assignment lemma that we reify is:

\begin{lstlisting}
Lemma semax_set_localD:
    forall temp var ret gt 
      id (e: Clight.expr) ty gs P T1 T2 R Post v,
  forall {Espec: OracleKind},
      typeof_temp (mk_tycontext temp var ret gt gs) id = Some ty -> 
      is_neutral_cast (implicit_deref (typeof e)) ty = true ->
      msubst_eval_LR T1 T2 e RRRR = Some v ->
      tc_expr_b_norho (mk_tycontext temp var ret gt gs) e = true ->
      assertD P (localD (PTree.set id v T1) T2) R = Post ->
      semax (mk_tycontext temp var ret gt gs) (|> (assertD P (localD T1 T2) R))
        (Sset id e)
          (normal_ret_assert Post).
Proof.
\end{lstlisting}

To apply this automatically, first the conclusion of the (reified)
lemma must match the proof goal, then we must solve each of the
subgoals that remain from the application. That means we must create
Mirror Core tactics that can solve each of the goals that will arise:

\begin{lstlisting}
Definition FORWARD_SET Delta Pre s :=
  let _HLIP :=
  match compute_hlip_arg (Delta, Pre, s) with
  | (temp, var, ret, gt, s, R) => HLIP tbl temp var ret gt R s
  end in
  let _APPLY_SET :=
  match compute_set_arg (Delta, Pre, s) with
  | Some (temp, var, ret, gt, i, e0, ty) =>
      THEN (EAPPLY typ func (set_lemma temp var ret gt i e0 ty))
           (TRY (FIRST [REFLEXIVITY_OP_CTYPE tbl;
                        REFLEXIVITY_MSUBST tbl; 
                        REFLEXIVITY_BOOL tbl;
                        AFTER_SET_LOAD tbl;
                        REFLEXIVITY tbl]))
  | _ => FAIL
  end in
  THEN _HLIP _APPLY_SET.
\end{lstlisting}


\subsection{Modular Reflection}

Before presenting the reflective framework we presented both the
typechecker, and a canonical form for assertions which allows us to do
substitutions computationally.  Both our techniques and the reflective
framework are reflective.  They use proved-sound Coq functions to make
progress in a proof. This section discusses how these techniques
interact. There is no interesting interaction between the substitution
and the typechecker because they occur in different places. When we
try to apply a reflective framework to a logic that uses these
reflective techniques, though, we run into some interesting
challenges. This section discusses those challenges and questions if
they are avoidable in a differently designed reflective framework.

A function that is used in a logic can fall into one of three
categories with respect to their interaction with the reflective
framework:

\begin{itemize}
\item A function that operates on constants and returns a type that
  can be represented as a constant requires no modification to be used
  in MirrorCore.
\item A function that operates on constants and returns Prop or Type
  will look exactly the same but return reified results.
\item A function that might operate on Coq variables must take reified
  expressions as arguments and return a reified result.
\end{itemize}

The first category is convenient, but unfortunately fairly few
functions that we use fall into this category. The reason that they
can return any type but Type or Prop is because in general if a
function returns a result we will want to reason about it, and if it
can't be represented by a constant we will be unable to do anything
with it in the Mirror-Core framework.

The second category includes the typechecker which returns
\lstinline|environ -> Prop| that might eventually be discharged by
either the person writing the proof or some automation.  In order to
allow reflective automation to have a chance to solve these remaining
conditions, we must write a new function who instead of returning
\lstinline|environ -> Prop| returns an \lstinline|expr| whose
denotation is (provably) the same as the result of the original
typechecker called on the same arguments.

\begin{minted}{coq}
Definition tc_expr_reif (e : c_expr) (Delta : tycontext) : expr.

Lemma tc_expr_reif_sound_complete : forall tus tvs e Delta,
exprD' tus tvs (tyarr tyenviron typrop) (tc_expr_reif e Delta) =
exprD' tus tbs (tyarr tyenviron typrop) (ftc_expr e Delta).
\end{minted}

Notice that the right hand side of the equality in the lemma doesn't
refer to the original typechecking function but the trivially reified
version. That is, \lstinline|ftc_expr| is a constructor whose
denotation is equal to \lstinline|tc_expr|, while
\lstinline|tc_expr_reif| is a function that when applied will have a
denotation equal to \lstinline|tc_expr| applied to the same
arguments. The function \lstinline|tc_expr_reif| can compute while no
progress can be made on \lstinline|ftc_expr|.  We put the denotation
function on both sides of the function because this allows us to write
an RTac that finds reified terms that look like \lstinline|ftc_expr e Delta|
(uncomputable) and replace them with a term 
\lstinline|tc_expr e Delta|. This is important because we might want to run some other
RTac on the result of the typechecking. Possibly an entailment solver
to discharge any remaining conditions.

We prove that \lstinline|tc_expr_reif| is sound, or that when it has a
denotation, that denotation matches the \lstinline|tc_expr|
function. We also prove it complete by showing that whenever
\lstinline|ftc_expr| has a denotation, \lstinline|tc_expr_reif| does
as well.  In this case sound and complete is the most useful so we
prove it. We have found other cases where only soundness is necessary,
and will discuss those later.

The final category is functions whose inputs might contain Coq
variables. An example of where we run into this is the
\lstinline|PTree.set| and \lstinline|PTree.get| operations that are in
the postcondition of our assignment rules. These functions can be
computed on their own, even if they are given a \lstinline|PTree| with
a coq variable in it as an argument. The problem is that there is no
way to represent any datastructure that contains a coq variable as a
constant. This is because there is no way to computationally compare
two Coq variables for equality. In order to computationally check
equality over Coq variables, they must first be reified.  Reification
is designed so that if it sees the same variable twice, it will use
the same constructor to represent both instances. The constructors can
be computationally compared, so variables can be compared in reified
syntax.  We will need to do comparisons on the variables in the local
assertion almost any time we solve an entailment, so we will need to
reify the variables.

What that means is that we must rewrite \lstinline|PTree.set| and
\lstinline|PTree.get| to operate on fully reified PTrees. This can be
difficult because of the size of the reified syntax. One way to
mitigate the complexity of the code is to create a function that
matches a reified expression as a tree:

\begin{minted}{coq}
Definition as_tree (e : expr typ func) : option
  ((typ * expr typ func * expr typ func * expr typ func) + typ) := 
match e with
  | (App (App (App (Inj (inr (Data (fnode t)))) l) o) r) =>
    Some (inl (t, l, o, r))
  | (Inj (inr (Data (fleaf t)))) =>
    Some (inr t)
  | _ => None
end.
\end{minted}

This allows the function that operates over reified expressions to look very
similar to original expression, and also simplifies the proof. The function
and the proof will now have 3 cases. The first have the same behavior (only reified)
as the \lstinline|leaf| and \lstinline|node| cases of the original function. The third
case is the case where \lstinline|as_tree| returns \lstinline|None|. This doesn't
mean that the function doesn't typecheck because there is no requirement on 
expressions that a PTree can only be represented by those two constructors. There
could be another constructor with the same denotation, or more likely a PTree
could be an application of \lstinline|PTree.set| that didn't get simplified.
If we encounter a reified PTree that isn't represented
only by the expected constructors, we have a choice depending on
weather we require completeness or not. If we require completeness,
we can just apply the set function as a reified constructor. This
will result in a complete function. That means that if we
pass our reified function all valid reified arguments
the denotation of the result of the function will always
match the denotation of the original function applied
to the denotation of the arguments. It is the most convenient
to write an RTac for reified functions of this type because
there is a pre-built RTac that replaces one reified expression
with another. This RTac can only be proved sound if there is a proof
of equality between any two expressions that the tactic might come
across. The only downside to this approach is that proving that the
reified function is complete takes much more work than only proving it
sound. 

These reified functions are an inconvenience and one of the largest
barriers to easily applying a reflective framework in a setting like
VST. It is an important open question weather this is avoidable. Can
a reflective framework reuse computations that might operate
over Coq variables without requiring reified functions? 


\chapter{Evaluation}

We combine the steps of reification, computation, and reflection
together with an application of the soundness proof of Rtac and our
symbolic execution function to create the \lstinline|rforward|, or
reflective forward, tactic that can operate directly on Coq proof
goals. Because it operates on proof goals, \lstinline|rforward| must
be programmed in Ltac.  It works by:
\begin{enumerate}
\item calling MirrorCore's Ocaml plugin to reify the goal,
\item calling \lstinline|vm_compute| on the symbolic execution Rtac
  applied to the reified goal,
\item applying the soundness lemma for the symbolic execution Rtac
  using the reified goal and the Rtac result as arguments, and
\item taking the denotation of the remaining subgoals.
\end{enumerate}
The key to making proof by reflection efficient is the use of the
\lstinline|vm_compute| tactic, which compiles bytecode and runs it in
a bytecode interpreter. The \lstinline|vm_compute| tactic is the
fastest way to do evaluation ($\beta\eta\delta$-normalization) in Coq
8.4; we expect the forthcoming \lstinline{native_compute} of Coq 8.5
will improve \emph{all} the results in this paper even more.  The use
of \lstinline|vm_compute| can do more harm than good in the wrong
situations. Calling it on subgoals with opaque variables can stop
functions from simplifying. If this happens deep in a stack of
function calls, resulting terms can become massive, and evaluation can
take an extremely long time. Therefore it is important to call
\lstinline|vm_compute| only on expressions that definitely evaluate to
a reasonably sized result. The reasonably sized result must assume
that all definitions that appear in the original expression will be
unfolded completely, meaning that there shouldn't be opaque terms
anywhere in a term VM compute is run on.
  
The test presented here is the symbolic execution of a program with
$n$ statements that assigns $0$ into $n$ different variables. This
test is actually a best case for the Ltac version because it is able
to notice that the precondition is always closed w.r.t the variable we
are assigning into and avoid introducing an existential at each
step. 
We show the performance of Ltac and
Rtac as $n$ increases in Figure \ref{fig:chart}.

\begin{figure}
\vspace{-2ex}
\includegraphics[width=\textwidth]{chart.pdf}
\vspace{-4ex}
\caption{Run time of Ltac vs Rtac, for $n$ consecutive C statements (log-log scale).}
\label{fig:chart}
\vspace{-4ex}
\end{figure}

Each data point is the average of 10 consecutive runs on a standard laptop. The
amount of memory on the laptop is over 2 gigabytes, which is the
maximum amount of memory that 32-bit Coq can use. Because it is very
difficult to get 64-bit Coq on all platforms, we attempt to write
tactics that can operate within the lower memory bounds.

Although this is a synthetic test, we consider
it a lower bound on the performance increase we will see in the
process of a real proof. This is because numerous other tests
indicate that for non-addressed variable assignments the size of the
precondition has the predominant effect on the run-time, not the
contents. This follows from what we would expect. The computation that
we need to do is almost all getting and setting from efficient tree
maps where get and set take $\log(n)$ time where $n$ is the largest
index that exists in the map. This is acceptable because we know that
our program input will be compiled by CompCert, and CompCert always
starts its variable indices with $1$, moving up sequentially. We can
be confident that this will always be the case because CompCert makes
use of PTrees. The remainder of the operations that RTac does are
linear (or slower) unifications between lemmas and proof goals, as
well as linear (or slower) rewrites by proved equalities. Repeated
linear operations are what lead to the polynomial operation we observe in
the chart above, not repeated $\log(n)$ gets and sets. 

The chart shows that there 
is a speedup of (typically) 40$\times$ (e.g., running $n\! = \! 8$ consecutive
C-program statements). 
Speedup improves with scale: $n\! =\! 50$ 
steps in Ltac takes over 2 minutes, while
the same number of steps in Rtac takes only .9 seconds, running almost 150$\times$ faster! 
Even for $n\!=\!1$ there is a 4$\times$ speedup.
This greatly improves interactivity of the logic. In general it
seems that growth of time relative to the number of program steps is
quadratic. Our benchmark that uses more complex preconditions
scales even better (not shown in a graph),
with 74$\times$ speedup at $n\! = \!10$.


The \emph{computation} curve, at the upper end, clearly
shows a time complexity of $n^{2.5}$.
This time complexity is roughly what we expect,
because (in this example) the
precondition grows linearly with the number of steps,
and each step has proof operations at least proportional to the size
of the precondition.



On a real-life example, a loop-body from 
OpenSSL's SHA function\footnote{The second loop body in
the \lstinline{sha256_block_data_order} function of the cited paper \cite{appel15:sha}; there are 850 nodes in the loop body's abstract synax tree.} 
takes 336 seconds to verify in our Ltac \lstinline{forward} tactic;
\lstinline|rforward| takes only 12 seconds---a 28$\times$ speedup. The sequence
includes 13 local-variable assignments,
5 loads, and 1 store, several of which contain 
huge mathematical expressions (resulting from macro-expansion in the
C source code). The assertions in that example
are large, with many local-variable and
spatial conjuncts.

Our computational theory of nested records yields 
a significant performance improvement.
Our \lstinline{data_at} benchmark verifies a sequence of 
4 loads and 4 stores
on a data structure represented by 8 spatial conjuncts,
taking 63 seconds in Ltac.  Using 
\lstinline{data_at}, with just one conjunct, 
verification takes 15 seconds in Ltac, a factor of 4 improvement.
We don't expect to see the same speedup in Rtac, which in
general is slower to be affected by changes in precondition size.

Unfortunately, after Rtac has efficiently computed a proof,
Coq's \lstinline|Qed| blows up, taking minutes  in some cases.
Qed blowups tend to occur when Coq cannot find an efficient
$\beta\eta$-conversion sequence to prove an equality
(even if the tactic script demonstrated one).
In the next section we discuss how to fix this.
Performance measurements in this section (for both Ltac and Rtac)
do not include Qed times; the Ltac Qed times are larger than
the Rtac Qed times, but both are terrible (e.g., 815 and 615 seconds,
respectively, for the SHA loop-body example).

While this is frustrating, as Coq continues to mature, even if Qed
time is slow, it won't cause too much frustration to the user. This is
because starting with Coq 8.5, proofs can be built and checked
asynchronously, allowing their statements to be used farther down in a
file while they build. A Qed time of a few minutes is inconsequential
as long as the user doesn't need to wait for it to continue proofs,
even proofs using the not-yet-checked proof. Furthermore, Coq 8.5 will allow for Coq .v
files to be compiled without checking proofs, making them very quickly
importable. Later it is possible to replace the unchecked object files
with object files. This means that libraries that are slow to build
can be built and usable rapidly, and then the actual work of checking
the proofs can be done in the background at the user's convenience.

\chapter{Conclusion}

\bibliographystyle{plain}
\bibliography{appel.bib}

\end{document}

